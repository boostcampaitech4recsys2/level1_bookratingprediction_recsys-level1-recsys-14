{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import time\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y)+self.eps)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FeaturesEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class FactorizationMachine_v(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Parameter(torch.rand(input_dim, latent_dim), requires_grad = True)\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        square_of_sum = torch.mm(x, self.v) ** 2\n",
    "        sum_of_square = torch.mm(x ** 2, self.v ** 2)\n",
    "        pair_interactions = torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)\n",
    "        output = linear + (0.5 * pair_interactions)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='parser')\n",
    "arg = parser.add_argument\n",
    "arg('--DATA_PATH', type=str, default='/opt/ml/input/code/data/', help='Data path를 설정할 수 있습니다.')\n",
    "args = parser.parse_args('')\n",
    "\n",
    "users = pd.read_csv(args.DATA_PATH + 'users.csv')\n",
    "books = pd.read_csv(args.DATA_PATH + 'books.csv')\n",
    "train = pd.read_csv(args.DATA_PATH + 'train_ratings.csv')\n",
    "test = pd.read_csv(args.DATA_PATH + 'test_ratings.csv')\n",
    "sub = pd.read_csv(args.DATA_PATH + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67544</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123629</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200273</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>210926</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        isbn  rating\n",
       "0        8  0002005018       4\n",
       "1    67544  0002005018       7\n",
       "2   123629  0002005018       8\n",
       "3   200273  0002005018       8\n",
       "4   210926  0002005018       9"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11676</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>116866</td>\n",
       "      <td>0002005018</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152827</td>\n",
       "      <td>0060973129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157969</td>\n",
       "      <td>0374157065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67958</td>\n",
       "      <td>0399135782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id        isbn  rating\n",
       "0    11676  0002005018       0\n",
       "1   116866  0002005018       0\n",
       "2   152827  0060973129       0\n",
       "3   157969  0374157065       0\n",
       "4    67958  0399135782       0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>location</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>timmins, ontario, canada</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11400</td>\n",
       "      <td>ottawa, ontario, canada</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11676</td>\n",
       "      <td>n/a, n/a, n/a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67544</td>\n",
       "      <td>toronto, ontario, canada</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85526</td>\n",
       "      <td>victoria, british columbia, canada</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>96054</td>\n",
       "      <td>ottawa, ontario, canada</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>116866</td>\n",
       "      <td>ottawa, ,</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>123629</td>\n",
       "      <td>kingston, ontario, canada</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177458</td>\n",
       "      <td>ottawa, ontario, canada</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>200273</td>\n",
       "      <td>comber, ontario, canada</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                            location   age\n",
       "0        8            timmins, ontario, canada   NaN\n",
       "1    11400             ottawa, ontario, canada  49.0\n",
       "2    11676                       n/a, n/a, n/a   NaN\n",
       "3    67544            toronto, ontario, canada  30.0\n",
       "4    85526  victoria, british columbia, canada  36.0\n",
       "5    96054             ottawa, ontario, canada  29.0\n",
       "6   116866                           ottawa, ,   NaN\n",
       "7   123629           kingston, ontario, canada   NaN\n",
       "8   177458             ottawa, ontario, canada  29.0\n",
       "9   200273             comber, ontario, canada   NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "\n",
    "idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "train['user_id'] = train['user_id'].map(user2idx)\n",
    "sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "\n",
    "train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "sub['isbn'] = sub['isbn'].map(isbn2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['rating'].max()\n",
    "train['rating'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  isbn  rating\n",
       "0        0     0       4\n",
       "1        1     0       7\n",
       "2        2     0       8\n",
       "3        3     0       8\n",
       "4        4     0       9\n",
       "5        5     0       7\n",
       "6        6     0       5\n",
       "7        7     1       8\n",
       "8        8     2       6\n",
       "9        9     2      10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10) # 왜 바꿨을까"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_ = books.copy()\n",
    "books_['isbn'] = books_['isbn'].map(isbn2idx) # books df 를 복사하고, isbns을 id로 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "df = copy.deepcopy(train)\n",
    "Train = False\n",
    "\n",
    "if Train == True:\n",
    "    df_ = df.copy()\n",
    "else:\n",
    "    df_ = df.copy()\n",
    "    df_['user_id'] = df_['user_id'].map(user2idx)\n",
    "    df_['isbn'] = df_['isbn'].map(isbn2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>62897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  isbn  rating\n",
       "0           NaN   NaN       4\n",
       "1           NaN   NaN       7\n",
       "2           NaN   NaN       8\n",
       "3           NaN   NaN       8\n",
       "4           NaN   NaN       9\n",
       "...         ...   ...     ...\n",
       "306790  62897.0   NaN       7\n",
       "306791      NaN   NaN       6\n",
       "306792      NaN   NaN       7\n",
       "306793      NaN   NaN       7\n",
       "306794      NaN   NaN      10\n",
       "\n",
       "[306795 rows x 3 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/EDA/text_model_practice.ipynb 셀 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m train \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     df_ \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "if train == True:\n",
    "    df_ = df.copy()\n",
    "else:\n",
    "    df_ = df.copy()\n",
    "    df_['user_id'] = df_['user_id'].map(user2idx)\n",
    "    df_['isbn'] = df_['isbn'].map(isbn2idx)\n",
    "\n",
    "df_ = pd.merge(df_, books_[['isbn', 'summary']], on='isbn', how='left')\n",
    "df_['summary'].fillna('None', inplace=True)\n",
    "df_['summary'] = df_['summary'].apply(lambda x:text_preprocessing(x))\n",
    "df_['summary'].replace({'':'None', ' ':'None'}, inplace=True)\n",
    "df_['summary_length'] = df_['summary'].apply(lambda x:len(x))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "if user_summary_merge_vector and item_summary_vector:\n",
    "    print('Create User Summary Merge Vector')\n",
    "    user_summary_merge_vector_list = []\n",
    "    for user in tqdm(df_['user_id'].unique()):\n",
    "        vector = text_to_vector(summary_merge(df_, user, 5), tokenizer, model, device)\n",
    "        user_summary_merge_vector_list.append(vector)\n",
    "    user_review_text_df = pd.DataFrame(df_['user_id'].unique(), columns=['user_id'])\n",
    "    user_review_text_df['user_summary_merge_vector'] = user_summary_merge_vector_list\n",
    "    vector = np.concatenate([\n",
    "                            user_review_text_df['user_id'].values.reshape(1, -1),\n",
    "                            user_review_text_df['user_summary_merge_vector'].values.reshape(1, -1)\n",
    "                            ])\n",
    "    if not os.path.exists('/opt/ml/input/code/data/text_vector'):\n",
    "        os.makedirs('/opt/ml/input/code/data/text_vector')\n",
    "    if train == True:\n",
    "        np.save('/opt/ml/input/code/data/text_vector/train_user_summary_merge_vector.npy', vector)\n",
    "    else:\n",
    "        np.save('/opt/ml/input/code/data/text_vector/test_user_summary_merge_vector.npy', vector)\n",
    "\n",
    "    print('Create Item Summary Vector')\n",
    "    item_summary_vector_list = []\n",
    "    books_text_df = df_[['isbn', 'summary']].copy()\n",
    "    books_text_df= books_text_df.drop_duplicates().reset_index(drop=True)\n",
    "    books_text_df['summary'].fillna('None', inplace=True)\n",
    "    for summary in tqdm(books_text_df['summary']):\n",
    "        vector = text_to_vector(summary, tokenizer, model, device)\n",
    "        item_summary_vector_list.append(vector)\n",
    "    books_text_df['item_summary_vector'] = item_summary_vector_list\n",
    "    vector = np.concatenate([\n",
    "                            books_text_df['isbn'].values.reshape(1, -1),\n",
    "                            books_text_df['item_summary_vector'].values.reshape(1, -1)\n",
    "                            ])\n",
    "    if not os.path.exists('./data/text_vector'):\n",
    "        os.makedirs('./data/text_vector')\n",
    "    if train == True:\n",
    "        np.save('./data/text_vector/train_item_summary_vector.npy', vector)\n",
    "    else:\n",
    "        np.save('./data/text_vector/test_item_summary_vector.npy', vector)\n",
    "else:\n",
    "    print('Check Vectorizer')\n",
    "    print('Vector Load')\n",
    "    if train == True:\n",
    "        user = np.load('/opt/ml/input/code/data/text_vector/train_user_summary_merge_vector.npy', allow_pickle=True)\n",
    "    else:\n",
    "        user = np.load('/opt/ml/input/code/data/text_vector/test_user_summary_merge_vector.npy', allow_pickle=True)\n",
    "    user_review_text_df = pd.DataFrame([user[0], user[1]]).T\n",
    "    user_review_text_df.columns = ['user_id', 'user_summary_merge_vector']\n",
    "    user_review_text_df['user_id'] = user_review_text_df['user_id'].astype('int')\n",
    "\n",
    "    if train == True:\n",
    "        item = np.load('data/text_vector/train_item_summary_vector.npy', allow_pickle=True)\n",
    "    else:\n",
    "        item = np.load('data/text_vector/test_item_summary_vector.npy', allow_pickle=True)\n",
    "    books_text_df = pd.DataFrame([item[0], item[1]]).T\n",
    "    books_text_df.columns = ['isbn', 'item_summary_vector']\n",
    "    books_text_df['isbn'] = books_text_df['isbn'].astype('int')\n",
    "\n",
    "\n",
    "df_ = pd.merge(df_, user_review_text_df, on='user_id', how='left')\n",
    "df_ = pd.merge(df_, books_text_df[['isbn', 'item_summary_vector']], on='isbn', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = process_text_data(\n",
    "    train,\n",
    "    books,\n",
    "    user2idx,\n",
    "    isbn2idx,\n",
    "    args.DEVICE,\n",
    "    train=True,\n",
    "    user_summary_merge_vector=args.DEEPCONN_VECTOR_CREATE,\n",
    "    item_summary_vector=args.DEEPCONN_VECTOR_CREATE\n",
    ")\n",
    "\n",
    "text_test = process_text_data(\n",
    "    test,\n",
    "    books,\n",
    "    user2idx,\n",
    "    isbn2idx,\n",
    "    args.DEVICE,\n",
    "    train=False, \n",
    "    user_summary_merge_vector=args.DEEPCONN_VECTOR_CREATE,\n",
    "    item_summary_vector=args.DEEPCONN_VECTOR_CREATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DATA_PATH='data/', MODEL=None, DATA_SHUFFLE=True, TEST_SIZE=0.2, SEED=42, BATCH_SIZE=1024, EPOCHS=10, LR=0.001, WEIGHT_DECAY=1e-06, DEVICE='cuda', FM_EMBED_DIM=16, FFM_EMBED_DIM=16, NCF_EMBED_DIM=16, NCF_MLP_DIMS=(16, 16), NCF_DROPOUT=0.2, WDN_EMBED_DIM=16, WDN_MLP_DIMS=(16, 16), WDN_DROPOUT=0.2, DCN_EMBED_DIM=16, DCN_MLP_DIMS=(16, 16), DCN_DROPOUT=0.2, DCN_NUM_LAYERS=3, CNN_FM_EMBED_DIM=128, CNN_FM_LATENT_DIM=8, DEEPCONN_VECTOR_CREATE=False, DEEPCONN_EMBED_DIM=32, DEEPCONN_LATENT_DIM=10, DEEPCONN_CONV_1D_OUT_DIM=50, DEEPCONN_KERNEL_SIZE=3, DEEPCONN_WORD_DIM=768, DEEPCONN_OUT_DIM=32)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "######################## BASIC ENVIRONMENT SETUP\n",
    "parser = argparse.ArgumentParser(description='parser')\n",
    "arg = parser.add_argument\n",
    "\n",
    "############### BASIC OPTION\n",
    "arg('--DATA_PATH', type=str, default='data/', help='Data path를 설정할 수 있습니다.')\n",
    "arg('--MODEL', type=str, choices=['FM', 'FFM', 'NCF', 'WDN', 'DCN', 'CNN_FM', 'DeepCoNN'],\n",
    "                            help='학습 및 예측할 모델을 선택할 수 있습니다.')\n",
    "arg('--DATA_SHUFFLE', type=bool, default=True, help='데이터 셔플 여부를 조정할 수 있습니다.')\n",
    "arg('--TEST_SIZE', type=float, default=0.2, help='Train/Valid split 비율을 조정할 수 있습니다.')\n",
    "arg('--SEED', type=int, default=42, help='seed 값을 조정할 수 있습니다.')\n",
    "\n",
    "############### TRAINING OPTION\n",
    "arg('--BATCH_SIZE', type=int, default=1024, help='Batch size를 조정할 수 있습니다.')\n",
    "arg('--EPOCHS', type=int, default=10, help='Epoch 수를 조정할 수 있습니다.')\n",
    "arg('--LR', type=float, default=1e-3, help='Learning Rate를 조정할 수 있습니다.')\n",
    "arg('--WEIGHT_DECAY', type=float, default=1e-6, help='Adam optimizer에서 정규화에 사용하는 값을 조정할 수 있습니다.')\n",
    "\n",
    "############### GPU\n",
    "arg('--DEVICE', type=str, default='cuda', choices=['cuda', 'cpu'], help='학습에 사용할 Device를 조정할 수 있습니다.')\n",
    "\n",
    "############### FM\n",
    "arg('--FM_EMBED_DIM', type=int, default=16, help='FM에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### FFM\n",
    "arg('--FFM_EMBED_DIM', type=int, default=16, help='FFM에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### NCF\n",
    "arg('--NCF_EMBED_DIM', type=int, default=16, help='NCF에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--NCF_MLP_DIMS', type=list, default=(16, 16), help='NCF에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--NCF_DROPOUT', type=float, default=0.2, help='NCF에서 Dropout rate를 조정할 수 있습니다.')\n",
    "\n",
    "############### WDN\n",
    "arg('--WDN_EMBED_DIM', type=int, default=16, help='WDN에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--WDN_MLP_DIMS', type=list, default=(16, 16), help='WDN에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--WDN_DROPOUT', type=float, default=0.2, help='WDN에서 Dropout rate를 조정할 수 있습니다.')\n",
    "\n",
    "############### DCN\n",
    "arg('--DCN_EMBED_DIM', type=int, default=16, help='DCN에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--DCN_MLP_DIMS', type=list, default=(16, 16), help='DCN에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--DCN_DROPOUT', type=float, default=0.2, help='DCN에서 Dropout rate를 조정할 수 있습니다.')\n",
    "arg('--DCN_NUM_LAYERS', type=int, default=3, help='DCN에서 Cross Network의 레이어 수를 조정할 수 있습니다.')\n",
    "\n",
    "############### CNN_FM\n",
    "arg('--CNN_FM_EMBED_DIM', type=int, default=128, help='CNN_FM에서 user와 item에 대한 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--CNN_FM_LATENT_DIM', type=int, default=8, help='CNN_FM에서 user/item/image에 대한 latent 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### DeepCoNN\n",
    "arg('--DEEPCONN_VECTOR_CREATE', type=bool, default=False, help='DEEP_CONN에서 text vector 생성 여부를 조정할 수 있으며 최초 학습에만 True로 설정하여야합니다.')\n",
    "arg('--DEEPCONN_EMBED_DIM', type=int, default=32, help='DEEP_CONN에서 user와 item에 대한 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_LATENT_DIM', type=int, default=10, help='DEEP_CONN에서 user/item/image에 대한 latent 차원을 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_CONV_1D_OUT_DIM', type=int, default=50, help='DEEP_CONN에서 1D conv의 출력 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_KERNEL_SIZE', type=int, default=3, help='DEEP_CONN에서 1D conv의 kernel 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_WORD_DIM', type=int, default=768, help='DEEP_CONN에서 1D conv의 입력 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_OUT_DIM', type=int, default=32, help='DEEP_CONN에서 1D conv의 출력 크기를 조정할 수 있습니다.')\n",
    "\n",
    "args = parser.parse_args('')\n",
    "\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "def text_preprocessing(summary):\n",
    "    summary = re.sub(\"[.,\\'\\\"''\"\"!?]\", \"\", summary)\n",
    "    summary = re.sub(\"[^0-9a-zA-Z\\\\s]\", \" \", summary)\n",
    "    summary = re.sub(\"\\s+\", \" \", summary)\n",
    "    summary = summary.lower()\n",
    "    return summary\n",
    "\n",
    "\n",
    "def summary_merge(df, user_id, max_summary):\n",
    "    return \" \".join(df[df['user_id'] == user_id].sort_values(by='summary_length', ascending=False)['summary'].values[:max_summary])\n",
    "# rating을 고려하는 방식으로\n",
    "\n",
    "def text_to_vector(text, tokenizer, model, device):\n",
    "    for sent in tokenize.sent_tokenize(text):\n",
    "        text_ = \"[CLS] \" + sent + \" [SEP]\"\n",
    "        tokenized = tokenizer.tokenize(text_)\n",
    "        indexed = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        segments_idx = [1] * len(tokenized)\n",
    "        token_tensor = torch.tensor([indexed])\n",
    "        sgments_tensor = torch.tensor([segments_idx])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_tensor.to(device), sgments_tensor.to(device))\n",
    "            encode_layers = outputs[0]\n",
    "            sentence_embedding = torch.mean(encode_layers[0], dim=0)\n",
    "    return sentence_embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def process_text_data(df, books, user2idx, isbn2idx, device, train=False, user_summary_merge_vector=False, item_summary_vector=False):\n",
    "    books_ = books.copy()\n",
    "    books_['isbn'] = books_['isbn'].map(isbn2idx)\n",
    "\n",
    "    if train == True:\n",
    "        df_ = df.copy()\n",
    "    else:\n",
    "        df_ = df.copy()\n",
    "        df_['user_id'] = df_['user_id'].map(user2idx)\n",
    "        df_['isbn'] = df_['isbn'].map(isbn2idx)\n",
    "\n",
    "    df_ = pd.merge(df_, books_[['isbn', 'summary']], on='isbn', how='left')\n",
    "    df_['summary'].fillna('None', inplace=True)\n",
    "    df_['summary'] = df_['summary'].apply(lambda x:text_preprocessing(x))\n",
    "    df_['summary'].replace({'':'None', ' ':'None'}, inplace=True)\n",
    "    df_['summary_length'] = df_['summary'].apply(lambda x:len(x))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "    if user_summary_merge_vector and item_summary_vector:\n",
    "        print('Create User Summary Merge Vector')\n",
    "        user_summary_merge_vector_list = []\n",
    "        for user in tqdm(df_['user_id'].unique()):\n",
    "            vector = text_to_vector(summary_merge(df_, user, 5), tokenizer, model, device)\n",
    "            user_summary_merge_vector_list.append(vector)\n",
    "        user_review_text_df = pd.DataFrame(df_['user_id'].unique(), columns=['user_id'])\n",
    "        user_review_text_df['user_summary_merge_vector'] = user_summary_merge_vector_list\n",
    "        vector = np.concatenate([\n",
    "                                user_review_text_df['user_id'].values.reshape(1, -1),\n",
    "                                user_review_text_df['user_summary_merge_vector'].values.reshape(1, -1)\n",
    "                                ])\n",
    "        if not os.path.exists('./data/text_vector'):\n",
    "            os.makedirs('./data/text_vector')\n",
    "        if train == True:\n",
    "            np.save('./data/text_vector/train_user_summary_merge_vector.npy', vector)\n",
    "        else:\n",
    "            np.save('./data/text_vector/test_user_summary_merge_vector.npy', vector)\n",
    "\n",
    "        print('Create Item Summary Vector')\n",
    "        item_summary_vector_list = []\n",
    "        books_text_df = df_[['isbn', 'summary']].copy()\n",
    "        books_text_df= books_text_df.drop_duplicates().reset_index(drop=True)\n",
    "        books_text_df['summary'].fillna('None', inplace=True)\n",
    "        for summary in tqdm(books_text_df['summary']):\n",
    "            vector = text_to_vector(summary, tokenizer, model, device)\n",
    "            item_summary_vector_list.append(vector)\n",
    "        books_text_df['item_summary_vector'] = item_summary_vector_list\n",
    "        vector = np.concatenate([\n",
    "                                books_text_df['isbn'].values.reshape(1, -1),\n",
    "                                books_text_df['item_summary_vector'].values.reshape(1, -1)\n",
    "                                ])\n",
    "        if not os.path.exists('./data/text_vector'):\n",
    "            os.makedirs('./data/text_vector')\n",
    "        if train == True:\n",
    "            np.save('./data/text_vector/train_item_summary_vector.npy', vector)\n",
    "        else:\n",
    "            np.save('./data/text_vector/test_item_summary_vector.npy', vector)\n",
    "    else:\n",
    "        print('Check Vectorizer')\n",
    "        print('Vector Load')\n",
    "        if train == True:\n",
    "            user = np.load('data/text_vector/train_user_summary_merge_vector.npy', allow_pickle=True)\n",
    "        else:\n",
    "            user = np.load('data/text_vector/test_user_summary_merge_vector.npy', allow_pickle=True)\n",
    "        user_review_text_df = pd.DataFrame([user[0], user[1]]).T\n",
    "        user_review_text_df.columns = ['user_id', 'user_summary_merge_vector']\n",
    "        user_review_text_df['user_id'] = user_review_text_df['user_id'].astype('int')\n",
    "\n",
    "        if train == True:\n",
    "            item = np.load('data/text_vector/train_item_summary_vector.npy', allow_pickle=True)\n",
    "        else:\n",
    "            item = np.load('data/text_vector/test_item_summary_vector.npy', allow_pickle=True)\n",
    "        books_text_df = pd.DataFrame([item[0], item[1]]).T\n",
    "        books_text_df.columns = ['isbn', 'item_summary_vector']\n",
    "        books_text_df['isbn'] = books_text_df['isbn'].astype('int')\n",
    "\n",
    "\n",
    "    df_ = pd.merge(df_, user_review_text_df, on='user_id', how='left')\n",
    "    df_ = pd.merge(df_, books_text_df[['isbn', 'item_summary_vector']], on='isbn', how='left')\n",
    "\n",
    "    return df_\n",
    "\n",
    "\n",
    "class Text_Dataset(Dataset):\n",
    "    def __init__(self, user_isbn_vector, user_summary_merge_vector, item_summary_vector, label):\n",
    "        self.user_isbn_vector = user_isbn_vector\n",
    "        self.user_summary_merge_vector = user_summary_merge_vector\n",
    "        self.item_summary_vector = item_summary_vector\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_isbn_vector.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "                'user_isbn_vector' : torch.tensor(self.user_isbn_vector[i], dtype=torch.long),\n",
    "                'user_summary_merge_vector' : torch.tensor(self.user_summary_merge_vector[i].reshape(-1, 1), dtype=torch.float32),\n",
    "                'item_summary_vector' : torch.tensor(self.item_summary_vector[i].reshape(-1, 1), dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label[i], dtype=torch.float32),\n",
    "                }\n",
    "\n",
    "\n",
    "def text_data_load(args):\n",
    "\n",
    "    users = pd.read_csv(args.DATA_PATH + 'users.csv')\n",
    "    books = pd.read_csv(args.DATA_PATH + 'books.csv')\n",
    "    train = pd.read_csv(args.DATA_PATH + 'train_ratings.csv')\n",
    "    test = pd.read_csv(args.DATA_PATH + 'test_ratings.csv')\n",
    "    sub = pd.read_csv(args.DATA_PATH + 'sample_submission.csv')\n",
    "\n",
    "    ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "    isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "\n",
    "    idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "    idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "    user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "    isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "    train['user_id'] = train['user_id'].map(user2idx)\n",
    "    sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "\n",
    "    train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "    sub['isbn'] = sub['isbn'].map(isbn2idx)\n",
    "\n",
    "    text_train = process_text_data(train, books, user2idx, isbn2idx, args.DEVICE, train=True, user_summary_merge_vector=args.DEEPCONN_VECTOR_CREATE, item_summary_vector=args.DEEPCONN_VECTOR_CREATE)\n",
    "    text_test = process_text_data(test, books, user2idx, isbn2idx, args.DEVICE, train=False, user_summary_merge_vector=args.DEEPCONN_VECTOR_CREATE, item_summary_vector=args.DEEPCONN_VECTOR_CREATE)\n",
    "\n",
    "    data = {\n",
    "            'train':train,\n",
    "            'test':test,\n",
    "            'users':users,\n",
    "            'books':books,\n",
    "            'sub':sub,\n",
    "            'idx2user':idx2user,\n",
    "            'idx2isbn':idx2isbn,\n",
    "            'user2idx':user2idx,\n",
    "            'isbn2idx':isbn2idx,\n",
    "            'text_train':text_train,\n",
    "            'text_test':text_test,\n",
    "            }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def text_data_split(args, data):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                        data['text_train'][['user_id', 'isbn', 'user_summary_merge_vector', 'item_summary_vector']],\n",
    "                                                        data['text_train']['rating'],\n",
    "                                                        test_size=args.TEST_SIZE,\n",
    "                                                        random_state=args.SEED,\n",
    "                                                        shuffle=True\n",
    "                                                        )\n",
    "    data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid\n",
    "    return data\n",
    "\n",
    "\n",
    "def text_data_loader(args, data):\n",
    "    train_dataset = Text_Dataset(\n",
    "                                data['X_train'][['user_id', 'isbn']].values,\n",
    "                                data['X_train']['user_summary_merge_vector'].values,\n",
    "                                data['X_train']['item_summary_vector'].values,\n",
    "                                data['y_train'].values\n",
    "                                )\n",
    "    valid_dataset = Text_Dataset(\n",
    "                                data['X_valid'][['user_id', 'isbn']].values,\n",
    "                                data['X_valid']['user_summary_merge_vector'].values,\n",
    "                                data['X_valid']['item_summary_vector'].values,\n",
    "                                data['y_valid'].values\n",
    "                                )\n",
    "    test_dataset = Text_Dataset(\n",
    "                                data['text_test'][['user_id', 'isbn']].values,\n",
    "                                data['text_test']['user_summary_merge_vector'].values,\n",
    "                                data['text_test']['item_summary_vector'].values,\n",
    "                                data['text_test']['rating'].values\n",
    "                                )\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.BATCH_SIZE, num_workers=0, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.BATCH_SIZE, num_workers=0, shuffle=False)\n",
    "    data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='parser')\n",
    "arg = parser.add_argument\n",
    "\n",
    "############### BASIC OPTION\n",
    "arg('--DATA_PATH', type=str, default='/opt/ml/input/code/data/', help='Data path를 설정할 수 있습니다.')\n",
    "arg('--MODEL', type=str, choices=['FM', 'FFM', 'NCF', 'WDN', 'DCN', 'CNN_FM', 'DeepCoNN'],\n",
    "                            help='학습 및 예측할 모델을 선택할 수 있습니다.')\n",
    "arg('--DATA_SHUFFLE', type=bool, default=True, help='데이터 셔플 여부를 조정할 수 있습니다.')\n",
    "arg('--TEST_SIZE', type=float, default=0.2, help='Train/Valid split 비율을 조정할 수 있습니다.')\n",
    "arg('--SEED', type=int, default=42, help='seed 값을 조정할 수 있습니다.')\n",
    "\n",
    "############### TRAINING OPTION\n",
    "arg('--BATCH_SIZE', type=int, default=1024, help='Batch size를 조정할 수 있습니다.')\n",
    "arg('--EPOCHS', type=int, default=10, help='Epoch 수를 조정할 수 있습니다.')\n",
    "arg('--LR', type=float, default=1e-3, help='Learning Rate를 조정할 수 있습니다.')\n",
    "arg('--WEIGHT_DECAY', type=float, default=1e-6, help='Adam optimizer에서 정규화에 사용하는 값을 조정할 수 있습니다.')\n",
    "\n",
    "############### GPU\n",
    "arg('--DEVICE', type=str, default='cuda', choices=['cuda', 'cpu'], help='학습에 사용할 Device를 조정할 수 있습니다.')\n",
    "\n",
    "############### FM\n",
    "arg('--FM_EMBED_DIM', type=int, default=16, help='FM에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### FFM\n",
    "arg('--FFM_EMBED_DIM', type=int, default=16, help='FFM에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### NCF\n",
    "arg('--NCF_EMBED_DIM', type=int, default=16, help='NCF에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--NCF_MLP_DIMS', type=list, default=(16, 16), help='NCF에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--NCF_DROPOUT', type=float, default=0.2, help='NCF에서 Dropout rate를 조정할 수 있습니다.')\n",
    "\n",
    "############### WDN\n",
    "arg('--WDN_EMBED_DIM', type=int, default=16, help='WDN에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--WDN_MLP_DIMS', type=list, default=(16, 16), help='WDN에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--WDN_DROPOUT', type=float, default=0.2, help='WDN에서 Dropout rate를 조정할 수 있습니다.')\n",
    "\n",
    "############### DCN\n",
    "arg('--DCN_EMBED_DIM', type=int, default=16, help='DCN에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--DCN_MLP_DIMS', type=list, default=(16, 16), help='DCN에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--DCN_DROPOUT', type=float, default=0.2, help='DCN에서 Dropout rate를 조정할 수 있습니다.')\n",
    "arg('--DCN_NUM_LAYERS', type=int, default=3, help='DCN에서 Cross Network의 레이어 수를 조정할 수 있습니다.')\n",
    "\n",
    "############### CNN_FM\n",
    "arg('--CNN_FM_EMBED_DIM', type=int, default=128, help='CNN_FM에서 user와 item에 대한 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--CNN_FM_LATENT_DIM', type=int, default=8, help='CNN_FM에서 user/item/image에 대한 latent 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### DeepCoNN\n",
    "arg('--DEEPCONN_VECTOR_CREATE', type=bool, default=False, help='DEEP_CONN에서 text vector 생성 여부를 조정할 수 있으며 최초 학습에만 True로 설정하여야합니다.')\n",
    "arg('--DEEPCONN_EMBED_DIM', type=int, default=32, help='DEEP_CONN에서 user와 item에 대한 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_LATENT_DIM', type=int, default=10, help='DEEP_CONN에서 user/item/image에 대한 latent 차원을 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_CONV_1D_OUT_DIM', type=int, default=50, help='DEEP_CONN에서 1D conv의 출력 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_KERNEL_SIZE', type=int, default=3, help='DEEP_CONN에서 1D conv의 kernel 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_WORD_DIM', type=int, default=768, help='DEEP_CONN에서 1D conv의 입력 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_OUT_DIM', type=int, default=32, help='DEEP_CONN에서 1D conv의 출력 크기를 조정할 수 있습니다.')\n",
    "\n",
    "args = parser.parse_args('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /opt/ml/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Vectorizer\n",
      "Vector Load\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/text_vector/train_user_summary_merge_vector.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/EDA/text_model_practice.ipynb 셀 20\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mpunkt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m text_data_load(args)\n",
      "\u001b[1;32m/opt/ml/input/EDA/text_model_practice.ipynb 셀 20\u001b[0m in \u001b[0;36mtext_data_load\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m train[\u001b[39m'\u001b[39m\u001b[39misbn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39misbn\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(isbn2idx)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m sub[\u001b[39m'\u001b[39m\u001b[39misbn\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m sub[\u001b[39m'\u001b[39m\u001b[39misbn\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(isbn2idx)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=167'>168</a>\u001b[0m text_train \u001b[39m=\u001b[39m process_text_data(train, books, user2idx, isbn2idx, args\u001b[39m.\u001b[39;49mDEVICE, train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, user_summary_merge_vector\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mDEEPCONN_VECTOR_CREATE, item_summary_vector\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mDEEPCONN_VECTOR_CREATE)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m text_test \u001b[39m=\u001b[39m process_text_data(test, books, user2idx, isbn2idx, args\u001b[39m.\u001b[39mDEVICE, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, user_summary_merge_vector\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mDEEPCONN_VECTOR_CREATE, item_summary_vector\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mDEEPCONN_VECTOR_CREATE)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=170'>171</a>\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=171'>172</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:train,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m:test,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=181'>182</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtext_test\u001b[39m\u001b[39m'\u001b[39m:text_test,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m         }\n",
      "\u001b[1;32m/opt/ml/input/EDA/text_model_practice.ipynb 셀 20\u001b[0m in \u001b[0;36mprocess_text_data\u001b[0;34m(df, books, user2idx, isbn2idx, device, train, user_summary_merge_vector, item_summary_vector)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mVector Load\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mif\u001b[39;00m train \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     user \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mdata/text_vector/train_user_summary_merge_vector.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/EDA/text_model_practice.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     user \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mdata/text_vector/test_user_summary_merge_vector.npy\u001b[39m\u001b[39m'\u001b[39m, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/numpy/lib/npyio.py:390\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    388\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    391\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/text_vector/train_user_summary_merge_vector.npy'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "data = text_data_load(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dongyoung')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f003a16f78e953d6c3fb089b790d98e4533a1b4d943e2d8ed222c282940149d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
