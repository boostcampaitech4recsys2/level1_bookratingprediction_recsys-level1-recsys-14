{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--code\n",
      "|    |--ensemble.py\n",
      "|    |--data\n",
      "|        |--train_ratings.csv\n",
      "|        |--test_ratings.csv\n",
      "|        |--books.csv\n",
      "|        |--sample_submission.csv\n",
      "|        |--images\n",
      "|            |--books.csv\n",
      "|        |--users.csv\n",
      "|        |--new_books.csv\n",
      "|        |--text_vector\n",
      "|    |--main.py\n",
      "|    |--models\n",
      "|    |--submit\n",
      "|        |--20221028_064428_DeepCoNN.csv\n",
      "|        |--20221028_154012_DeepCoNN.csv\n",
      "|        |--DeepCoNN 64 100 1e-3 cuda 16 16 64_3.466269941396977.csv\n",
      "|        |--20221030_030447_DeepCoNN_2.196782058477402.csv\n",
      "|        |--NCF 128 0.005 1e-3 5_None.csv\n",
      "|        |--20221030_032151_DeepCoNN_2.198073727885882.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-5_2.19326662098368.csv\n",
      "|        |--_not_modified-mixed.csv\n",
      "|        |--128 0.001 1e-05_2.67.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8 --WEIGHT_DECAY 1e-5_2.1927552555998164.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 1e-4_2.1836092439790566.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8 --WEIGHT_DECAY 1e-6_2.1927552555998164.csv\n",
      "|        |--20221029_135716_DeepCoNN_2.1845559241871038-20221029_141405_DeepCoNN_2.195366261402766-20221029_114640_DeepCoNN_2.19169407064716-20221028_155816_DeepCoNN_2.17-aw.csv\n",
      "|        |--_modified-aw.csv\n",
      "|        |--NCF 128 0.005 2e-3 1_None.csv\n",
      "|        |--FFM_None.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 1e-6_2.1836092439790566.csv\n",
      "|        |--20221028_153130_DeepCoNN.csv\n",
      "|        |--DeepCoNN 512 100 1e-2 cuda 8 8 32_2.3633410579708.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-6_2.19326662098368.csv\n",
      "|        |--MIX_128_0.0005_0.0008_2.154307274414532_no_image.csv\n",
      "|        |--20221028_101046_DeepCoNN.csv\n",
      "|        |--20221029_202940_DeepCoNN_2.19326662098368.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-5_2.1845559241871038.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-4_2.19326662098368.csv\n",
      "|        |--_modified-mixed.csv\n",
      "|        |--20221028_155816_DeepCoNN_2.17.csv\n",
      "|        |--NCF_128_0.003_2e-3_1_None.csv\n",
      "|        |--20221029_135716_DeepCoNN_2.1845559241871038.csv\n",
      "|        |--20221028_060640_DeepCoNN.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8 --WEIGHT_DECAY 1e-6_2.19169407064716.csv\n",
      "|        |--NCF 128 0.005 7e-4 5_None.csv\n",
      "|        |--FFM_128_0.005_2e-4_20_2.1707715799015004.csv\n",
      "|        |--MIX_32_0.001_0.0004_2.1580838039474832.csv\n",
      "|        |--333.csv\n",
      "|        |--MIX_128_0.004_0.0004_2.1562050003155093_sub.csv\n",
      "|        |--111.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-6_2.1845559241871038.csv\n",
      "|        |--20221029_120347_DeepCoNN_2.1999752536416053.csv\n",
      "|        |--222.csv\n",
      "|        |--20221026_071627_CNN_FM.csv\n",
      "|        |--444.csv\n",
      "|        |--20221028_050902_DeepCoNN.csv\n",
      "|        |--20221029_204654_DeepCoNN_2.1921184703707697.csv\n",
      "|        |--20221030_053251_DeepCoNN_2.1968285103638965.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 1e-5_2.1836092439790566.csv\n",
      "|        |--FFM_128_0.005_2e-4_30_2.1707715799015004.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8 --WEIGHT_DECAY 1e-4_2.19169407064716.csv\n",
      "|        |--20221028_051840_DeepCoNN.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 2048 --EPOCHS 1_5.868888473510742.csv\n",
      "|        |--FFM 128 0.01 1e-4_None.csv\n",
      "|        |--20221026_075921_CNN_FM.csv\n",
      "|        |--20221029_141405_DeepCoNN_2.195366261402766.csv\n",
      "|        |--NCF_128_0.003_2e-3_3_None.csv\n",
      "|        |--20221030_051606_DeepCoNN_2.199996263285478.csv\n",
      "|        |--DeepCoNN 128 100 1e-3 cuda 64 64 64_2.4156085350508327.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-4_2.1845559241871038.csv\n",
      "|        |--555.csv\n",
      "|        |--20221029_114640_DeepCoNN_2.19169407064716.csv\n",
      "|        |--20221029_181837_DeepCoNN_2.1927552555998164.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8 --WEIGHT_DECAY 1e-5_2.19169407064716.csv\n",
      "|        |--20221026_080222_CNN_FM.csv\n",
      "|        |--20221026_072937_CNN_FM.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 1e-4_2.1913868224782576.csv\n",
      "|        |--6-aw.csv\n",
      "|        |--20221026_075103_CNN_FM.csv\n",
      "|        |--20221026_073811_CNN_FM.csv\n",
      "|        |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8 --WEIGHT_DECAY 1e-4_2.1927552555998164.csv\n",
      "|        |--FFM_128_0.003_2e-3_3_2.2903010202668916.csv\n",
      "|        |--20221029_224058_DeepCoNN_2.1856900230050087.csv\n",
      "|        |--MIX_128_0.004_0.0004_2.146704132571204_sub.csv\n",
      "|    |--src\n",
      "|        |--data\n",
      "|            |--dl_data.py\n",
      "|            |--context_data.py\n",
      "|            |--__init__.py\n",
      "|            |--image_data.py\n",
      "|            |--text_data.py\n",
      "|            |--__pycache__\n",
      "|        |--utils.py\n",
      "|        |--ensembles\n",
      "|            |--ensembles.py\n",
      "|            |--__pycache__\n",
      "|        |--models\n",
      "|            |--dl_models.py\n",
      "|            |--context_models.py\n",
      "|            |--text_models.py\n",
      "|            |--image_models.py\n",
      "|            |--_models.py\n",
      "|            |--__pycache__\n",
      "|        |--__init__.py\n",
      "|        |--__pycache__\n",
      "|    |--MIXMODEL-aw.csv\n",
      "|--QA\n",
      "|    |--data\n",
      "|--exper_logs\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 64\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 64\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 64\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 64\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 64\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|--exper_logs3\n",
      "|    |----ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"303d5493-fc7b-4b78-a728-f9654590082b\" --shell=9002 --transport=\"tcp\" --iopub=9004 --f=\n",
      "|        |--opt\n",
      "|            |--ml\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 32 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 5e-4\n",
      "|    |----ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"8b831d80-327c-467c-97de-8d0921d9a813\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=\n",
      "|        |--opt\n",
      "|            |--ml\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 1e-3 --DEVICE cuda --DEEPCONN_EMBED_DIM 64 --DEEPCONN_LATENT_DIM 64 --DEEPCONN_CONV_1D_OUT_DIM 64\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 64 --DEEPCONN_LATENT_DIM 64 --DEEPCONN_CONV_1D_OUT_DIM 64 --WEIGHT_DECAY 5e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 1e-5\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 2048 --EPOCHS 1 --LR 0.01 --DEEPCONN_EMBED_DIM 32 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 5e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 1e-3 --DEVICE cuda --DEEPCONN_EMBED_DIM 16 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 64\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-5 --DEEPCONN_VECTOR_CREATE True\n",
      "|    |----ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"152de6f4-49e5-41f5-9c22-e57409bd7eb3\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=\n",
      "|        |--opt\n",
      "|            |--ml\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 16 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 1e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 64 --EPOCHS 100 --LR 0.001 --DEEPCONN_EMBED_DIM 32 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 5e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 32 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 7e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 1e-2 --DEVICE cuda --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 1e-3 --DEVICE cuda --DEEPCONN_EMBED_DIM 32 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 1 --LR 0.01 --DEEPCONN_EMBED_DIM 256 --DEEPCONN_LATENT_DIM 256 --DEEPCONN_CONV_1D_OUT_DIM 256 --WEIGHT_DECAY 5e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 5e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 32 --EPOCHS 100 --LR 1e-3 --DEVICE cuda --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32 --DEEPCONN_KERNEL_SIZE 5\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 2048 --EPOCHS 1\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 1024 --EPOCHS 1 --LR 0.01 --DEEPCONN_EMBED_DIM 32 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 5e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32 --WEIGHT_DECAY 1e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 256 --DEEPCONN_LATENT_DIM 256 --DEEPCONN_CONV_1D_OUT_DIM 256 --WEIGHT_DECAY 5e-4\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16 --WEIGHT_DECAY 1e-5\n",
      "|    |----ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"c8e5583c-286d-4579-ad9f-1b349237f493\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=\n",
      "|        |--opt\n",
      "|            |--ml\n",
      "|--models\n",
      "|--EDA\n",
      "|--exper_logs2\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 16 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 128 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.01 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 32\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 32 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 512 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 16\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.05 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 16 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "|    |----MODEL DeepCoNN --BATCH_SIZE 256 --EPOCHS 100 --LR 0.005 --DEEPCONN_EMBED_DIM 8 --DEEPCONN_LATENT_DIM 8 --DEEPCONN_CONV_1D_OUT_DIM 8\n",
      "\n",
      " 164 directiories, 94 files\n"
     ]
    }
   ],
   "source": [
    "import glob, os.path\n",
    "ndir = nfile = 0\n",
    "\n",
    "def traverse(dir, depth):\n",
    "    global ndir, nfile\n",
    "\n",
    "    for obj in glob.glob(dir + '/*'):\n",
    "        if depth == 0:\n",
    "            prefix = '|--'\n",
    "\n",
    "        else:\n",
    "            prefix = '|' + '    ' * depth + '|--'\n",
    "\n",
    "        if os.path.isdir(obj):  # 디렉터리인 경우\n",
    "            ndir += 1\n",
    "            print(prefix + os.path.basename(obj))\n",
    "            traverse(obj, depth + 1)\n",
    "\n",
    "        elif os.path.isfile(obj):\n",
    "            if str(os.path.basename(obj))[-3:] == '.py':\n",
    "                nfile += 1\n",
    "                print(prefix + os.path.basename(obj))\n",
    "            elif str(os.path.basename(obj))[-4:] == '.csv':\n",
    "                nfile += 1\n",
    "                print(prefix + os.path.basename(obj))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    traverse('.', 0)\n",
    "    print('\\n',ndir,'directiories,',nfile,'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='parser')\n",
    "arg = parser.add_argument\n",
    "\n",
    "############### BASIC OPTION\n",
    "arg('--DATA_PATH', type=str, default='/opt/ml/input/code/data/', help='Data path를 설정할 수 있습니다.')\n",
    "arg('--MODEL', type=str, choices=['FM', 'FFM', 'NCF', 'WDN', 'DCN', 'CNN_FM', 'DeepCoNN'],\n",
    "                            help='학습 및 예측할 모델을 선택할 수 있습니다.')\n",
    "arg('--DATA_SHUFFLE', type=bool, default=True, help='데이터 셔플 여부를 조정할 수 있습니다.')\n",
    "arg('--TEST_SIZE', type=float, default=0.2, help='Train/Valid split 비율을 조정할 수 있습니다.')\n",
    "arg('--SEED', type=int, default=42, help='seed 값을 조정할 수 있습니다.')\n",
    "\n",
    "############### TRAINING OPTION\n",
    "arg('--BATCH_SIZE', type=int, default=1024, help='Batch size를 조정할 수 있습니다.')\n",
    "arg('--EPOCHS', type=int, default=50, help='Epoch 수를 조정할 수 있습니다.')\n",
    "arg('--LR', type=float, default=0.0007, help='Learning Rate를 조정할 수 있습니다.')\n",
    "arg('--WEIGHT_DECAY', type=float, default=0.004, help='Adam optimizer에서 정규화에 사용하는 값을 조정할 수 있습니다.')\n",
    "\n",
    "############### GPU\n",
    "arg('--DEVICE', type=str, default='cuda', choices=['cuda', 'cpu'], help='학습에 사용할 Device를 조정할 수 있습니다.')\n",
    "\n",
    "############### FM\n",
    "arg('--FM_EMBED_DIM', type=int, default=16, help='FM에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### FFM\n",
    "arg('--FFM_EMBED_DIM', type=int, default=16, help='FFM에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### NCF\n",
    "arg('--NCF_EMBED_DIM', type=int, default=16, help='NCF에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--NCF_MLP_DIMS', type=list, default=(16, 16), help='NCF에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--NCF_DROPOUT', type=float, default=0.2, help='NCF에서 Dropout rate를 조정할 수 있습니다.')\n",
    "\n",
    "############### WDN\n",
    "arg('--WDN_EMBED_DIM', type=int, default=16, help='WDN에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--WDN_MLP_DIMS', type=list, default=(16, 16), help='WDN에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--WDN_DROPOUT', type=float, default=0.2, help='WDN에서 Dropout rate를 조정할 수 있습니다.')\n",
    "\n",
    "############### DCN\n",
    "arg('--DCN_EMBED_DIM', type=int, default=16, help='DCN에서 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--DCN_MLP_DIMS', type=list, default=(16, 16), help='DCN에서 MLP Network의 차원을 조정할 수 있습니다.')\n",
    "arg('--DCN_DROPOUT', type=float, default=0.2, help='DCN에서 Dropout rate를 조정할 수 있습니다.')\n",
    "arg('--DCN_NUM_LAYERS', type=int, default=3, help='DCN에서 Cross Network의 레이어 수를 조정할 수 있습니다.')\n",
    "\n",
    "############### CNN_FM\n",
    "arg('--CNN_FM_EMBED_DIM', type=int, default=128, help='CNN_FM에서 user와 item에 대한 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--CNN_FM_LATENT_DIM', type=int, default=8, help='CNN_FM에서 user/item/image에 대한 latent 차원을 조정할 수 있습니다.')\n",
    "\n",
    "############### DeepCoNN\n",
    "arg('--DEEPCONN_VECTOR_CREATE', type=bool, default=False, help='DEEP_CONN에서 text vector 생성 여부를 조정할 수 있으며 최초 학습에만 True로 설정하여야합니다.')\n",
    "arg('--DEEPCONN_EMBED_DIM', type=int, default=8, help='DEEP_CONN에서 user와 item에 대한 embedding시킬 차원을 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_LATENT_DIM', type=int, default=8, help='DEEP_CONN에서 user/item/image에 대한 latent 차원을 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_CONV_1D_OUT_DIM', type=int, default=16, help='DEEP_CONN에서 1D conv의 출력 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_KERNEL_SIZE', type=int, default=3, help='DEEP_CONN에서 1D conv의 kernel 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_WORD_DIM', type=int, default=768, help='DEEP_CONN에서 1D conv의 입력 크기를 조정할 수 있습니다.')\n",
    "arg('--DEEPCONN_OUT_DIM', type=int, default=16, help='DEEP_CONN에서 1D conv의 출력 크기를 조정할 수 있습니다.')\n",
    "\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "\n",
    "def image_vector(path):\n",
    "    img = Image.open(path)\n",
    "    scale = transforms.Resize((32, 32))\n",
    "    tensor = transforms.ToTensor()\n",
    "    img_fe = Variable(tensor(scale(img)))\n",
    "    return img_fe\n",
    "\n",
    "def process_img_data(df, books, user2idx, isbn2idx, train=False):\n",
    "    books_ = books.copy()\n",
    "    books_['isbn'] = books_['isbn'].map(isbn2idx)\n",
    "\n",
    "    if train == True:\n",
    "        df_ = df.copy()\n",
    "    else:\n",
    "        df_ = df.copy()\n",
    "        df_['user_id'] = df_['user_id'].map(user2idx)\n",
    "        df_['isbn'] = df_['isbn'].map(isbn2idx)\n",
    "\n",
    "    df_ = pd.merge(df_, books_[['isbn', 'img_path']], on='isbn', how='left')\n",
    "    df_['img_path'] = df_['img_path'].apply(lambda x: '/opt/ml/input/code/data/'+x)\n",
    "    img_vector_df = df_[['img_path']].drop_duplicates().reset_index(drop=True).copy()\n",
    "    data_box = []\n",
    "    for idx, path in tqdm(enumerate(sorted(img_vector_df['img_path']))):\n",
    "        data = image_vector(path)\n",
    "        if data.size()[0] == 3:\n",
    "            data_box.append(np.array(data))\n",
    "        else:\n",
    "            data_box.append(np.array(data.expand(3, data.size()[1], data.size()[2])))\n",
    "    img_vector_df['img_vector'] = data_box\n",
    "    df_ = pd.merge(df_, img_vector_df, on='img_path', how='left')\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "129777it [00:56, 2284.66it/s]\n",
      "52000it [00:23, 2235.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "users = pd.read_csv(args.DATA_PATH + 'users.csv')\n",
    "books = pd.read_csv(args.DATA_PATH + 'new_books.csv')\n",
    "train = pd.read_csv(args.DATA_PATH + 'train_ratings.csv')\n",
    "test = pd.read_csv(args.DATA_PATH + 'test_ratings.csv')\n",
    "sub = pd.read_csv(args.DATA_PATH + 'sample_submission.csv')\n",
    "bfip = pd.read_csv(args.DATA_PATH + 'books.csv')\n",
    "\n",
    "\n",
    "ids = pd.concat([train['user_id'], sub['user_id']]).unique()\n",
    "isbns = pd.concat([train['isbn'], sub['isbn']]).unique()\n",
    "\n",
    "# average_ratings = train.groupby('isbn')['rating'].mean()\n",
    "\n",
    "avg = dict() \n",
    "for i, isbn in enumerate(isbns):\n",
    "    if isbn in avg:\n",
    "        avg[isbn][0] += train['rating'][i]\n",
    "        avg[isbn][1] += 1\n",
    "    else:\n",
    "        avg[isbn] = [train['rating'][i], 1]\n",
    "\n",
    "r_avg = dict() # train에 나온 책들의 평균 rating raw_isbn -> rating\n",
    "for key in avg.keys():\n",
    "    r_avg[key] = avg[key][0] / avg[key][1]\n",
    "\n",
    "idx2user = {idx:id for idx, id in enumerate(ids)}\n",
    "idx2isbn = {idx:isbn for idx, isbn in enumerate(isbns)}\n",
    "\n",
    "user2idx = {id:idx for idx, id in idx2user.items()}\n",
    "isbn2idx = {isbn:idx for idx, isbn in idx2isbn.items()}\n",
    "\n",
    "train['user_id'] = train['user_id'].map(user2idx)\n",
    "sub['user_id'] = sub['user_id'].map(user2idx)\n",
    "\n",
    "train['isbn'] = train['isbn'].map(isbn2idx)\n",
    "sub['isbn'] = sub['isbn'].map(isbn2idx)\n",
    "\n",
    "img_train = process_img_data(train, bfip, user2idx, isbn2idx, train=True)\n",
    "img_test = process_img_data(test, bfip, user2idx, isbn2idx, train=False) # 여기서 image train 데이터프레임 생긴 거 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> int64 [     8  67544 123629 200273 210926]\n",
      "<class 'numpy.ndarray'> object ['0002005018' '0060973129' '0374157065' '0399135782' '0425176428']\n",
      "<class 'dict'> 8\n",
      "<class 'dict'> 0060973129\n",
      "<class 'dict'> 2\n",
      "<class 'dict'> 3\n",
      "   user_id  isbn  rating\n",
      "0        0     0       4\n",
      "1        1     0       7\n",
      "2        2     0       8\n",
      "3        3     0       8\n",
      "4        4     0       9\n",
      "   user_id  isbn  rating\n",
      "0       13     0       0\n",
      "1    13426     0       0\n",
      "2    26761     1       0\n",
      "3    16495     2       0\n",
      "4     6225     3       0\n",
      "   user_id        isbn  rating\n",
      "0    11676  0002005018       0\n",
      "1   116866  0002005018       0\n",
      "2   152827  0060973129       0\n",
      "3   157969  0374157065       0\n",
      "4    67958  0399135782       0\n"
     ]
    }
   ],
   "source": [
    "print(type(ids), ids.dtype, ids[:5])\n",
    "print(type(isbns), isbns.dtype, isbns[:5])\n",
    "print(type(idx2user), idx2user[0])\n",
    "print(type(idx2isbn), idx2isbn[1])\n",
    "print(type(user2idx), user2idx[idx2user[2]])\n",
    "print(type(isbn2idx), isbn2idx[idx2isbn[3]])\n",
    "print(train.head())\n",
    "print(sub.head())\n",
    "print(test.head()) # train, sub는 idx, test 는 raw 로 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(summary):\n",
    "    summary = re.sub(\"[.,\\'\\\"''\"\"!?]\", \"\", summary)\n",
    "    summary = re.sub(\"[^0-9a-zA-Z\\\\s]\", \" \", summary)\n",
    "    summary = re.sub(\"\\s+\", \" \", summary)\n",
    "    summary = summary.lower()\n",
    "    return summary\n",
    "\n",
    "\n",
    "def summary_merge(df, user_id, max_summary):\n",
    "    return \" \".join(df[df['user_id'] == user_id].sort_values(by='summary_length', ascending=False)['summary'].values[:max_summary])\n",
    "# rating을 고려하는 방식으로\n",
    "\n",
    "\n",
    "def text_to_vector(text, tokenizer, model, device):\n",
    "    for sent in tokenize.sent_tokenize(text):\n",
    "        text_ = \"[CLS] \" + sent + \" [SEP]\"\n",
    "        tokenized = tokenizer.tokenize(text_)\n",
    "        indexed = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        segments_idx = [1] * len(tokenized)\n",
    "        token_tensor = torch.tensor([indexed])\n",
    "        sgments_tensor = torch.tensor([segments_idx])\n",
    "        with torch.no_grad():\n",
    "            outputs = model(token_tensor.to(device), sgments_tensor.to(device))\n",
    "            encode_layers = outputs[0]\n",
    "            sentence_embedding = torch.mean(encode_layers[0], dim=0)\n",
    "    return sentence_embedding.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "def process_text_data(df, books, user2idx, isbn2idx, device, train=False, user_summary_merge_vector=False, item_summary_vector=False):\n",
    "    books_ = books.copy()\n",
    "    books_['isbn'] = books_['isbn'].map(isbn2idx)\n",
    "\n",
    "    if train == True:\n",
    "        df_ = df.copy()\n",
    "    else:\n",
    "        df_ = df.copy()\n",
    "        df_['user_id'] = df_['user_id'].map(user2idx)\n",
    "        df_['isbn'] = df_['isbn'].map(isbn2idx)\n",
    "\n",
    "    df_ = pd.merge(df_, books_[['isbn', 'summary']], on='isbn', how='left')\n",
    "    df_['summary'].fillna('None', inplace=True)\n",
    "    df_['summary'] = df_['summary'].apply(lambda x:text_preprocessing(x))\n",
    "    df_['summary'].replace({'':'None', ' ':'None'}, inplace=True)\n",
    "    df_['summary_length'] = df_['summary'].apply(lambda x:len(x))\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "    if user_summary_merge_vector and item_summary_vector:\n",
    "        print('Create User Summary Merge Vector')\n",
    "        user_summary_merge_vector_list = []\n",
    "        for user in tqdm(df_['user_id'].unique()):\n",
    "            vector = text_to_vector(summary_merge(df_, user, 5), tokenizer, model, device)\n",
    "            user_summary_merge_vector_list.append(vector)\n",
    "        user_review_text_df = pd.DataFrame(df_['user_id'].unique(), columns=['user_id'])\n",
    "        user_review_text_df['user_summary_merge_vector'] = user_summary_merge_vector_list\n",
    "        vector = np.concatenate([\n",
    "                                user_review_text_df['user_id'].values.reshape(1, -1),\n",
    "                                user_review_text_df['user_summary_merge_vector'].values.reshape(1, -1)\n",
    "                                ])\n",
    "        if not os.path.exists('/opt/ml/input/code/data/text_vector'):\n",
    "            os.makedirs('/opt/ml/input/code/data/text_vector')\n",
    "        if train == True:\n",
    "            np.save('/opt/ml/input/code/data/text_vector/train_user_summary_merge_vector.npy', vector)\n",
    "        else:\n",
    "            np.save('/opt/ml/input/code/data/text_vector/test_user_summary_merge_vector.npy', vector)\n",
    "\n",
    "        print('Create Item Summary Vector')\n",
    "        item_summary_vector_list = []\n",
    "        books_text_df = df_[['isbn', 'summary']].copy()\n",
    "        books_text_df= books_text_df.drop_duplicates().reset_index(drop=True)\n",
    "        books_text_df['summary'].fillna('None', inplace=True)\n",
    "        for summary in tqdm(books_text_df['summary']):\n",
    "            vector = text_to_vector(summary, tokenizer, model, device)\n",
    "            item_summary_vector_list.append(vector)\n",
    "        books_text_df['item_summary_vector'] = item_summary_vector_list\n",
    "        vector = np.concatenate([\n",
    "                                books_text_df['isbn'].values.reshape(1, -1),\n",
    "                                books_text_df['item_summary_vector'].values.reshape(1, -1)\n",
    "                                ])\n",
    "        if not os.path.exists('/opt/ml/input/code/data/text_vector'):\n",
    "            os.makedirs('/opt/ml/input/code/data/text_vector')\n",
    "        if train == True:\n",
    "            np.save('/opt/ml/input/code/data/text_vector/train_item_summary_vector.npy', vector)\n",
    "        else:\n",
    "            np.save('/opt/ml/input/code/data/text_vector/test_item_summary_vector.npy', vector)\n",
    "    else:\n",
    "        print('Check Vectorizer')\n",
    "        print('Vector Load')\n",
    "        if train == True:\n",
    "            user = np.load('/opt/ml/input/code/data/text_vector/train_user_summary_merge_vector.npy', allow_pickle=True)\n",
    "        else:\n",
    "            user = np.load('/opt/ml/input/code/data/text_vector/test_user_summary_merge_vector.npy', allow_pickle=True)\n",
    "        user_review_text_df = pd.DataFrame([user[0], user[1]]).T\n",
    "        user_review_text_df.columns = ['user_id', 'user_summary_merge_vector']\n",
    "        user_review_text_df['user_id'] = user_review_text_df['user_id'].astype('int')\n",
    "\n",
    "        if train == True:\n",
    "            item = np.load('/opt/ml/input/code/data/text_vector/train_item_summary_vector.npy', allow_pickle=True)\n",
    "        else:\n",
    "            item = np.load('/opt/ml/input/code/data/text_vector/test_item_summary_vector.npy', allow_pickle=True)\n",
    "        books_text_df = pd.DataFrame([item[0], item[1]]).T\n",
    "        books_text_df.columns = ['isbn', 'item_summary_vector']\n",
    "        books_text_df['isbn'] = books_text_df['isbn'].astype('int')\n",
    "\n",
    "\n",
    "    df_ = pd.merge(df_, user_review_text_df, on='user_id', how='left')\n",
    "    df_ = pd.merge(df_, books_text_df[['isbn', 'item_summary_vector']], on='isbn', how='left')\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Vectorizer\n",
      "Vector Load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Vectorizer\n",
      "Vector Load\n"
     ]
    }
   ],
   "source": [
    "text_train = process_text_data(\n",
    "        train,\n",
    "        books,\n",
    "        user2idx,\n",
    "        isbn2idx,\n",
    "        args.DEVICE,\n",
    "        train=True,\n",
    "        user_summary_merge_vector=args.DEEPCONN_VECTOR_CREATE,\n",
    "        item_summary_vector=args.DEEPCONN_VECTOR_CREATE\n",
    ")\n",
    "\n",
    "text_test = process_text_data(\n",
    "        test,\n",
    "        books,\n",
    "        user2idx,\n",
    "        isbn2idx,\n",
    "        args.DEVICE,\n",
    "        train=False,\n",
    "        user_summary_merge_vector=args.DEEPCONN_VECTOR_CREATE,\n",
    "        item_summary_vector=args.DEEPCONN_VECTOR_CREATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68092\n"
     ]
    }
   ],
   "source": [
    "print(len(data['users']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "   user_id  isbn  rating  age  location_city  location_state  \\\n",
      "0        0     0       4    3              0               0   \n",
      "1        1     0       7    3              1               0   \n",
      "2        2     0       8    3              2               0   \n",
      "3        3     0       8    3              3               0   \n",
      "4        4     0       9    3              4               0   \n",
      "\n",
      "   location_country  category  publisher  language  book_author  \\\n",
      "0                 0         0          0         0            0   \n",
      "1                 0         0          0         0            0   \n",
      "2                 0         0          0         0            0   \n",
      "3                 0         0          0         0            0   \n",
      "4                 0         0          0         0            0   \n",
      "\n",
      "                                             summary  summary_length  \\\n",
      "0  in a small town in canada clara callan relucta...             107   \n",
      "1  in a small town in canada clara callan relucta...             107   \n",
      "2  in a small town in canada clara callan relucta...             107   \n",
      "3  in a small town in canada clara callan relucta...             107   \n",
      "4  in a small town in canada clara callan relucta...             107   \n",
      "\n",
      "                           user_summary_merge_vector  \\\n",
      "0  [-0.16397206, 0.08885264, 0.381835, -0.1191637...   \n",
      "1  [-0.2748889, -0.015643405, 0.44334778, -0.0183...   \n",
      "2  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "3  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "4  [-0.42387947, 0.14355494, 0.1418772, -0.232609...   \n",
      "\n",
      "                                 item_summary_vector  \\\n",
      "0  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "1  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "2  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "3  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "4  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "\n",
      "                                          img_vector  \n",
      "0  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
      "1  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
      "2  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
      "3  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
      "4  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
      "test\n",
      "   user_id  isbn  age  location_city  location_state  location_country  \\\n",
      "0       13     0    3             13               8                 2   \n",
      "1    13426     0    3            309              12                 3   \n",
      "2    26761     1    4            309               0                 0   \n",
      "3    16495     2    3            195              15                 1   \n",
      "4     6225     3    3           2717              94                 1   \n",
      "\n",
      "   category  publisher  language  book_author  rating  \\\n",
      "0         0          0         0            0       0   \n",
      "1         0          0         0            0       0   \n",
      "2         1          1         0            1       0   \n",
      "3         2          2         0            2       0   \n",
      "4         3          3         0            3       0   \n",
      "\n",
      "                                             summary  summary_length  \\\n",
      "0  in a small town in canada clara callan relucta...             107   \n",
      "1  in a small town in canada clara callan relucta...             107   \n",
      "2  here for the first time in paperback is an out...             235   \n",
      "3  describes the great flu epidemic of 1918 an ou...             218   \n",
      "4  a chinese immigrant who is convinced she is dy...             195   \n",
      "\n",
      "                           user_summary_merge_vector  \\\n",
      "0  [-0.30769682, 0.0151690515, 0.3217908, -0.1050...   \n",
      "1  [-0.20867082, 0.3197794, 0.13658687, -0.140712...   \n",
      "2  [-0.5034143, 0.16244616, 0.0833784, -0.0654003...   \n",
      "3  [-0.05029916, 0.130561, -0.17901792, -0.152968...   \n",
      "4  [0.030334461, 0.29610607, 0.40644652, -0.13158...   \n",
      "\n",
      "                                 item_summary_vector  \\\n",
      "0  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "1  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
      "2  [-0.5386576, 0.12224596, 0.078437544, -0.05627...   \n",
      "3  [-0.05029916, 0.130561, -0.17901792, -0.152968...   \n",
      "4  [0.124538615, 0.2809713, 0.23661214, -0.281009...   \n",
      "\n",
      "                                          img_vector  \n",
      "0  [[[0.99215686, 0.99607843, 0.9882353, 0.996078...  \n",
      "1  [[[0.99215686, 0.99607843, 0.9882353, 0.996078...  \n",
      "2  [[[0.42352942, 0.5921569, 0.6039216, 0.4156862...  \n",
      "3  [[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...  \n",
      "4  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
      "users\n",
      "   user_id                            location   age location_city  \\\n",
      "0      0.0            timmins, ontario, canada   NaN       timmins   \n",
      "1   2334.0             ottawa, ontario, canada  49.0        ottawa   \n",
      "2     13.0                       n/a, n/a, n/a   NaN           n/a   \n",
      "3      1.0            toronto, ontario, canada  30.0       toronto   \n",
      "4   4789.0  victoria, british columbia, canada  36.0      victoria   \n",
      "\n",
      "      location_state location_country  \n",
      "0            ontario           canada  \n",
      "1            ontario           canada  \n",
      "2                n/a              n/a  \n",
      "3            ontario           canada  \n",
      "4   british columbia           canada  \n",
      "books\n",
      "   Unnamed: 0  isbn                                         book_title  \\\n",
      "0           0     0                                       Clara Callan   \n",
      "1           1     1                               Decision in Normandy   \n",
      "2           2     2  Flu: The Story of the Great Influenza Pandemic...   \n",
      "3           3     3                             The Kitchen God's Wife   \n",
      "4           4     4  What If?: The World's Foremost Military Histor...   \n",
      "\n",
      "          book_author  year_of_publication                 publisher  \\\n",
      "0  richardbrucewright               2001.0     HarperFlamingo Canada   \n",
      "1         carlod'este               1991.0           HarperPerennial   \n",
      "2      ginabarikolata               1999.0      Farrar Straus Giroux   \n",
      "3              amytan               1991.0          Putnam Pub Group   \n",
      "4        robertcowley               2000.0  Berkley Publishing Group   \n",
      "\n",
      "                                             img_url  language   category  \\\n",
      "0  http://images.amazon.com/images/P/0002005018.0...         1  actresses   \n",
      "1  http://images.amazon.com/images/P/0060973129.0...         1     others   \n",
      "2  http://images.amazon.com/images/P/0374157065.0...         1    medical   \n",
      "3  http://images.amazon.com/images/P/0399135782.0...         1    fiction   \n",
      "4  http://images.amazon.com/images/P/0425176428.0...         1    history   \n",
      "\n",
      "                                             summary  \\\n",
      "0  In a small town in Canada, Clara Callan reluct...   \n",
      "1  Here, for the first time in paperback, is an o...   \n",
      "2  Describes the great flu epidemic of 1918, an o...   \n",
      "3  A Chinese immigrant who is convinced she is dy...   \n",
      "4  Essays by respected military historians, inclu...   \n",
      "\n",
      "                            img_path  \n",
      "0  images/0002005018.01.THUMBZZZ.jpg  \n",
      "1  images/0060973129.01.THUMBZZZ.jpg  \n",
      "2  images/0374157065.01.THUMBZZZ.jpg  \n",
      "3  images/0399135782.01.THUMBZZZ.jpg  \n",
      "4  images/0425176428.01.THUMBZZZ.jpg  \n",
      "sub\n",
      "   user_id  isbn  rating\n",
      "0       13     0       0\n",
      "1    13426     0       0\n",
      "2    26761     1       0\n",
      "3    16495     2       0\n",
      "4     6225     3       0\n"
     ]
    }
   ],
   "source": [
    "print('train')\n",
    "print(data['train'].head())\n",
    "print('test')\n",
    "print(data['test'].head()) # raw data\n",
    "print('users')\n",
    "print(data['users'].head())\n",
    "print('books')\n",
    "print(data['books'].head())\n",
    "print('sub')\n",
    "print(data['sub'].head()) # idx로 되어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['img_vector'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>user_summary_merge_vector</th>\n",
       "      <th>item_summary_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.16397206, 0.08885264, 0.381835, -0.1191637...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.2748889, -0.015643405, 0.44334778, -0.0183...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.42387947, 0.14355494, 0.1418772, -0.232609...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  isbn  rating                                            summary  \\\n",
       "0        0     0       4  in a small town in canada clara callan relucta...   \n",
       "1        1     0       7  in a small town in canada clara callan relucta...   \n",
       "2        2     0       8  in a small town in canada clara callan relucta...   \n",
       "3        3     0       8  in a small town in canada clara callan relucta...   \n",
       "4        4     0       9  in a small town in canada clara callan relucta...   \n",
       "\n",
       "   summary_length                          user_summary_merge_vector  \\\n",
       "0             107  [-0.16397206, 0.08885264, 0.381835, -0.1191637...   \n",
       "1             107  [-0.2748889, -0.015643405, 0.44334778, -0.0183...   \n",
       "2             107  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
       "3             107  [-0.21812883, -0.20391987, 0.06439817, -0.1359...   \n",
       "4             107  [-0.42387947, 0.14355494, 0.1418772, -0.232609...   \n",
       "\n",
       "                                 item_summary_vector  \n",
       "0  [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "1  [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "2  [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "3  [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "4  [-0.21812883, -0.20391987, 0.06439817, -0.1359...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_train'].head() # 이거 어떻게 만들어진거지? preprocess 에서 나온거같은데"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>user_summary_merge_vector</th>\n",
       "      <th>item_summary_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.30769682, 0.0151690515, 0.3217908, -0.1050...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.20867082, 0.3197794, 0.13658687, -0.140712...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>here for the first time in paperback is an out...</td>\n",
       "      <td>235</td>\n",
       "      <td>[-0.5034143, 0.16244616, 0.0833784, -0.0654003...</td>\n",
       "      <td>[-0.5386576, 0.12224596, 0.078437544, -0.05627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16495</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>describes the great flu epidemic of 1918 an ou...</td>\n",
       "      <td>218</td>\n",
       "      <td>[-0.05029916, 0.130561, -0.17901792, -0.152968...</td>\n",
       "      <td>[-0.05029916, 0.130561, -0.17901792, -0.152968...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6225</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>199</td>\n",
       "      <td>[0.030334461, 0.29610607, 0.40644652, -0.13158...</td>\n",
       "      <td>[0.124538615, 0.2809713, 0.23661214, -0.281009...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  isbn  rating                                            summary  \\\n",
       "0       13     0       0  in a small town in canada clara callan relucta...   \n",
       "1    13426     0       0  in a small town in canada clara callan relucta...   \n",
       "2    26761     1       0  here for the first time in paperback is an out...   \n",
       "3    16495     2       0  describes the great flu epidemic of 1918 an ou...   \n",
       "4     6225     3       0  a chinese immigrant who is convinced she is dy...   \n",
       "\n",
       "   summary_length                          user_summary_merge_vector  \\\n",
       "0             107  [-0.30769682, 0.0151690515, 0.3217908, -0.1050...   \n",
       "1             107  [-0.20867082, 0.3197794, 0.13658687, -0.140712...   \n",
       "2             235  [-0.5034143, 0.16244616, 0.0833784, -0.0654003...   \n",
       "3             218  [-0.05029916, 0.130561, -0.17901792, -0.152968...   \n",
       "4             199  [0.030334461, 0.29610607, 0.40644652, -0.13158...   \n",
       "\n",
       "                                 item_summary_vector  \n",
       "0  [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "1  [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "2  [-0.5386576, 0.12224596, 0.078437544, -0.05627...  \n",
       "3  [-0.05029916, 0.130561, -0.17901792, -0.152968...  \n",
       "4  [0.124538615, 0.2809713, 0.23661214, -0.281009...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id    isbn                          user_summary_merge_vector  \\\n",
      "121312    22946   15310  [-0.22506751, 0.029421099, 0.3118938, 0.055340...   \n",
      "265089    44452   91405  [-0.32664108, -0.033238977, 0.49888423, -0.173...   \n",
      "60236     15913    4800  [-0.19206597, 0.17479151, 0.38801938, -0.14827...   \n",
      "111218     1339   13043  [-0.16191524, 0.062218465, 0.31686616, -0.0634...   \n",
      "306001    15663  128985  [-0.31652385, 0.08598539, 0.19682412, 0.016981...   \n",
      "\n",
      "                                      item_summary_vector  age  location_city  \\\n",
      "121312  [-0.3385258, -0.12692931, -0.1336062, -0.24439...    3            865   \n",
      "265089  [0.11896893, -0.4549427, -0.6011843, -0.158617...    6             70   \n",
      "60236   [-0.09902138, 0.26748508, 0.122802645, -0.0946...    3           1018   \n",
      "111218  [0.0035208128, -0.14371993, 0.28286156, 0.0871...    2            912   \n",
      "306001  [-0.0793449, -0.25213027, -0.44699535, 0.04909...    4           4779   \n",
      "\n",
      "        location_state  location_country  category  publisher  language  \\\n",
      "121312              12                 3         3        131         0   \n",
      "265089              14                 1         5       4327         0   \n",
      "60236               15                 1         8        220         0   \n",
      "111218              40                 1         3          4         0   \n",
      "306001             420                 3         5        652         0   \n",
      "\n",
      "        book_author                                         img_vector  \n",
      "121312         7133  [[[0.003921569, 0.003921569, 0.003921569, 0.00...  \n",
      "265089        36283  [[[0.75686276, 0.81960785, 0.8980392, 0.874509...  \n",
      "60236          1724  [[[0.06666667, 0.09411765, 0.08235294, 0.03529...  \n",
      "111218          505  [[[0.9098039, 0.9137255, 0.9098039, 0.89411765...  \n",
      "306001        52329  [[[0.90588236, 0.81960785, 0.827451, 0.8235294...  \n",
      "\n",
      "len(train_user): 52486\n",
      "[0, 1, 3, 4, 5]\n",
      "\n",
      "len(train_isbn): 111369\n",
      "[0, 1, 2, 3, 4]\n",
      "\n",
      "len(valid_user): 22440\n",
      "[0, 1, 2, 4, 5]\n",
      "\n",
      "len(valid_isbn): 39334\n",
      "[0, 2, 3, 7, 9]\n",
      "\n",
      "len(valid_user - train_user): 7317\n",
      "cold valid user percent: 32.61%\n",
      "\n",
      "len(valid_isbn - train_isbn): 18408\n",
      "cold valid isbn percent: 46.80%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    data['train'][['user_id', 'isbn', 'user_summary_merge_vector', 'item_summary_vector', 'age', 'location_city', 'location_state',\n",
    "       'location_country', 'category', 'publisher', 'language', 'book_author', 'img_vector']],\n",
    "    data['train']['rating'],\n",
    "    test_size=args.TEST_SIZE,\n",
    "    random_state=args.SEED,\n",
    "    shuffle=True,\n",
    ")\n",
    "data['X_train'], data['X_valid'], data['y_train'], data['y_valid'] = X_train, X_valid, y_train, y_valid\n",
    "\n",
    "print(X_train.head(5)) # shuffle해서 index가 이상해짐 (0, 1, 2, 3 ,,, X)\n",
    "print()\n",
    "\n",
    "train_user = set(X_train['user_id'].to_list()) # train에는 있는 user\n",
    "print(f'len(train_user): {len(train_user)}') # train-val 분할 뒤에 train에만 있는 user\n",
    "print(list(train_user)[:5])\n",
    "print()\n",
    "\n",
    "train_isbn = set(X_train['isbn'])\n",
    "print(f'len(train_isbn): {len(train_isbn)}') # train-val 분할 뒤에 train에만 있는 isbn\n",
    "print(list(train_isbn)[:5])\n",
    "print()\n",
    "\n",
    "valid_user = set(X_valid['user_id'].to_list()) # train에는 있는 user\n",
    "print(f'len(valid_user): {len(valid_user)}') # train-val 분할 뒤에 train에만 있는 user\n",
    "print(list(valid_user)[:5])\n",
    "print()\n",
    "\n",
    "valid_isbn = set(X_valid['isbn'].to_list())\n",
    "print(f'len(valid_isbn): {len(valid_isbn)}') # train-val 분할 뒤에 train에만 있는 isbn\n",
    "print(list(valid_isbn)[:5])\n",
    "print()\n",
    "\n",
    "print(f'len(valid_user - train_user): {len(valid_user - train_user)}')\n",
    "print(f'cold valid user percent: {len(valid_user - train_user) * 100 / len(valid_user):.2f}%')\n",
    "print()\n",
    "\n",
    "print(f'len(valid_isbn - train_isbn): {len(valid_isbn - train_isbn)}')\n",
    "print(f'cold valid isbn percent: {len(valid_isbn - train_isbn) * 100 / len(valid_isbn):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'field_dims', 'users', 'books', 'sub', 'idx2user', 'idx2isbn', 'user2idx', 'isbn2idx', 'img_train', 'img_test', 'X_train', 'X_valid', 'y_train', 'y_valid', 'train_dataloader', 'valid_dataloader', 'test_dataloader'])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16495</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6225</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>7728</td>\n",
       "      <td>149565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76695</th>\n",
       "      <td>47785</td>\n",
       "      <td>149566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76696</th>\n",
       "      <td>4209</td>\n",
       "      <td>149567</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>40779</td>\n",
       "      <td>149568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76698</th>\n",
       "      <td>1879</td>\n",
       "      <td>149569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76699 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id    isbn  rating\n",
       "0           13       0       0\n",
       "1        13426       0       0\n",
       "2        26761       1       0\n",
       "3        16495       2       0\n",
       "4         6225       3       0\n",
       "...        ...     ...     ...\n",
       "76694     7728  149565       0\n",
       "76695    47785  149566       0\n",
       "76696     4209  149567       0\n",
       "76697    40779  149568       0\n",
       "76698     1879  149569       0\n",
       "\n",
       "[76699 rows x 3 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sub'] # id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>category</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>book_author</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>user_summary_merge_vector</th>\n",
       "      <th>item_summary_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.30769682, 0.0151690515, 0.3217908, -0.1050...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13426</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>309</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.20867082, 0.3197794, 0.13658687, -0.140712...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26761</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>309</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>here for the first time in paperback is an out...</td>\n",
       "      <td>235</td>\n",
       "      <td>[-0.5034143, 0.16244616, 0.0833784, -0.0654003...</td>\n",
       "      <td>[-0.5386576, 0.12224596, 0.078437544, -0.05627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16495</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>195</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>describes the great flu epidemic of 1918 an ou...</td>\n",
       "      <td>218</td>\n",
       "      <td>[-0.05029916, 0.130561, -0.17901792, -0.152968...</td>\n",
       "      <td>[-0.05029916, 0.130561, -0.17901792, -0.152968...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6225</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2717</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>195</td>\n",
       "      <td>[0.030334461, 0.29610607, 0.40644652, -0.13158...</td>\n",
       "      <td>[0.124538615, 0.2809713, 0.23661214, -0.281009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>7728</td>\n",
       "      <td>149565</td>\n",
       "      <td>3</td>\n",
       "      <td>3088</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>1214</td>\n",
       "      <td>0</td>\n",
       "      <td>on becoming childwise responds to this need by...</td>\n",
       "      <td>156</td>\n",
       "      <td>[-0.15238021, 0.5389414, -0.10356555, -0.10182...</td>\n",
       "      <td>[-0.15238021, 0.5389414, -0.10356555, -0.10182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76695</th>\n",
       "      <td>47785</td>\n",
       "      <td>149566</td>\n",
       "      <td>3</td>\n",
       "      <td>323</td>\n",
       "      <td>106</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>10249</td>\n",
       "      <td>0</td>\n",
       "      <td>respektlos macht der autor mit der griechische...</td>\n",
       "      <td>157</td>\n",
       "      <td>[-0.6279584, 0.1575934, 0.19220233, -0.2471607...</td>\n",
       "      <td>[-0.64320105, 0.06612672, 0.114306755, -0.2445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76696</th>\n",
       "      <td>4209</td>\n",
       "      <td>149567</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7784</td>\n",
       "      <td>0</td>\n",
       "      <td>59825</td>\n",
       "      <td>0</td>\n",
       "      <td>the fascinating characters in this short story...</td>\n",
       "      <td>177</td>\n",
       "      <td>[-0.38790068, 0.11098844, 0.27980253, -0.12648...</td>\n",
       "      <td>[0.11831131, 0.26141438, 0.47210327, -0.080064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>40779</td>\n",
       "      <td>149568</td>\n",
       "      <td>4</td>\n",
       "      <td>210</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5046</td>\n",
       "      <td>1</td>\n",
       "      <td>59826</td>\n",
       "      <td>0</td>\n",
       "      <td>la muerte del decano</td>\n",
       "      <td>20</td>\n",
       "      <td>[-0.066250935, -0.11153858, 0.22444469, -0.003...</td>\n",
       "      <td>[-0.07091211, -0.1524299, -0.28767318, -0.0033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76698</th>\n",
       "      <td>1879</td>\n",
       "      <td>149569</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>59827</td>\n",
       "      <td>0</td>\n",
       "      <td>a daring twist on the travel adventure genre t...</td>\n",
       "      <td>226</td>\n",
       "      <td>[-0.38273704, 0.12612452, 0.4502024, -0.041709...</td>\n",
       "      <td>[-0.3169796, 0.19284458, 0.42234662, -0.121430...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76699 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id    isbn  age  location_city  location_state  location_country  \\\n",
       "0           13       0    3             13               8                 2   \n",
       "1        13426       0    3            309              12                 3   \n",
       "2        26761       1    4            309               0                 0   \n",
       "3        16495       2    3            195              15                 1   \n",
       "4         6225       3    3           2717              94                 1   \n",
       "...        ...     ...  ...            ...             ...               ...   \n",
       "76694     7728  149565    3           3088               5                 1   \n",
       "76695    47785  149566    3            323             106                19   \n",
       "76696     4209  149567    3             20              19                 1   \n",
       "76697    40779  149568    4            210              67                14   \n",
       "76698     1879  149569    3             19              10                 1   \n",
       "\n",
       "       category  publisher  language  book_author  rating  \\\n",
       "0             0          0         0            0       0   \n",
       "1             0          0         0            0       0   \n",
       "2             1          1         0            1       0   \n",
       "3             2          2         0            2       0   \n",
       "4             3          3         0            3       0   \n",
       "...         ...        ...       ...          ...     ...   \n",
       "76694        50        310         0         1214       0   \n",
       "76695         1        108         1        10249       0   \n",
       "76696         3       7784         0        59825       0   \n",
       "76697         5       5046         1        59826       0   \n",
       "76698         7        224         0        59827       0   \n",
       "\n",
       "                                                 summary  summary_length  \\\n",
       "0      in a small town in canada clara callan relucta...             107   \n",
       "1      in a small town in canada clara callan relucta...             107   \n",
       "2      here for the first time in paperback is an out...             235   \n",
       "3      describes the great flu epidemic of 1918 an ou...             218   \n",
       "4      a chinese immigrant who is convinced she is dy...             195   \n",
       "...                                                  ...             ...   \n",
       "76694  on becoming childwise responds to this need by...             156   \n",
       "76695  respektlos macht der autor mit der griechische...             157   \n",
       "76696  the fascinating characters in this short story...             177   \n",
       "76697                               la muerte del decano              20   \n",
       "76698  a daring twist on the travel adventure genre t...             226   \n",
       "\n",
       "                               user_summary_merge_vector  \\\n",
       "0      [-0.30769682, 0.0151690515, 0.3217908, -0.1050...   \n",
       "1      [-0.20867082, 0.3197794, 0.13658687, -0.140712...   \n",
       "2      [-0.5034143, 0.16244616, 0.0833784, -0.0654003...   \n",
       "3      [-0.05029916, 0.130561, -0.17901792, -0.152968...   \n",
       "4      [0.030334461, 0.29610607, 0.40644652, -0.13158...   \n",
       "...                                                  ...   \n",
       "76694  [-0.15238021, 0.5389414, -0.10356555, -0.10182...   \n",
       "76695  [-0.6279584, 0.1575934, 0.19220233, -0.2471607...   \n",
       "76696  [-0.38790068, 0.11098844, 0.27980253, -0.12648...   \n",
       "76697  [-0.066250935, -0.11153858, 0.22444469, -0.003...   \n",
       "76698  [-0.38273704, 0.12612452, 0.4502024, -0.041709...   \n",
       "\n",
       "                                     item_summary_vector  \n",
       "0      [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "1      [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "2      [-0.5386576, 0.12224596, 0.078437544, -0.05627...  \n",
       "3      [-0.05029916, 0.130561, -0.17901792, -0.152968...  \n",
       "4      [0.124538615, 0.2809713, 0.23661214, -0.281009...  \n",
       "...                                                  ...  \n",
       "76694  [-0.15238021, 0.5389414, -0.10356555, -0.10182...  \n",
       "76695  [-0.64320105, 0.06612672, 0.114306755, -0.2445...  \n",
       "76696  [0.11831131, 0.26141438, 0.47210327, -0.080064...  \n",
       "76697  [-0.07091211, -0.1524299, -0.28767318, -0.0033...  \n",
       "76698  [-0.3169796, 0.19284458, 0.42234662, -0.121430...  \n",
       "\n",
       "[76699 rows x 15 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['test'] # id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['books']['summary'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(data['books']['summary'].isnull().sum() / len(data['books']['summary']) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['books']['summary'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>img_url</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>richardbrucewright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>actresses</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>carlod'este</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>others</td>\n",
       "      <td>Here, for the first time in paperback, is an o...</td>\n",
       "      <td>images/0060973129.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>ginabarikolata</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>medical</td>\n",
       "      <td>Describes the great flu epidemic of 1918, an o...</td>\n",
       "      <td>images/0374157065.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>amytan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>fiction</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>images/0399135782.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What If?: The World's Foremost Military Histor...</td>\n",
       "      <td>robertcowley</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0425176428.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>history</td>\n",
       "      <td>Essays by respected military historians, inclu...</td>\n",
       "      <td>images/0425176428.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149516</th>\n",
       "      <td>129773</td>\n",
       "      <td>The Bachelor Home Companion: A Practical Guide...</td>\n",
       "      <td>p.j.o'rourke</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Pocket Books</td>\n",
       "      <td>http://images.amazon.com/images/P/067161746X.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>humor</td>\n",
       "      <td>A tongue-in-cheek survival guide for single pe...</td>\n",
       "      <td>images/067161746X.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149517</th>\n",
       "      <td>149569</td>\n",
       "      <td>All Elevations Unknown: An Adventure in the He...</td>\n",
       "      <td>samlightner</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Broadway Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0767907566.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>nature</td>\n",
       "      <td>A daring twist on the travel-adventure genre t...</td>\n",
       "      <td>images/0767907566.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149518</th>\n",
       "      <td>129774</td>\n",
       "      <td>Why stop?: A guide to Texas historical roadsid...</td>\n",
       "      <td>claudedooley</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Lone Star Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0884159221.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>Why stop?: A guide to Texas historical roadsid...</td>\n",
       "      <td>images/0884159221.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149519</th>\n",
       "      <td>129775</td>\n",
       "      <td>The Are You Being Served? Stories: 'Camping In...</td>\n",
       "      <td>jeremylloyd</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Kqed Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0912333022.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>fiction</td>\n",
       "      <td>These hilarious stories by the creator of publ...</td>\n",
       "      <td>images/0912333022.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149520</th>\n",
       "      <td>129776</td>\n",
       "      <td>Dallas Street Map Guide and Directory, 2000 Ed...</td>\n",
       "      <td>mapsco</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>American Map Corporation</td>\n",
       "      <td>http://images.amazon.com/images/P/1569661057.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>Dallas Street Map Guide and Directory, 2000 Ed...</td>\n",
       "      <td>images/1569661057.01.THUMBZZZ.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149521 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          isbn                                         book_title  \\\n",
       "0            0                                       Clara Callan   \n",
       "1            1                               Decision in Normandy   \n",
       "2            2  Flu: The Story of the Great Influenza Pandemic...   \n",
       "3            3                             The Kitchen God's Wife   \n",
       "4            4  What If?: The World's Foremost Military Histor...   \n",
       "...        ...                                                ...   \n",
       "149516  129773  The Bachelor Home Companion: A Practical Guide...   \n",
       "149517  149569  All Elevations Unknown: An Adventure in the He...   \n",
       "149518  129774  Why stop?: A guide to Texas historical roadsid...   \n",
       "149519  129775  The Are You Being Served? Stories: 'Camping In...   \n",
       "149520  129776  Dallas Street Map Guide and Directory, 2000 Ed...   \n",
       "\n",
       "               book_author  year_of_publication                 publisher  \\\n",
       "0       richardbrucewright               2001.0     HarperFlamingo Canada   \n",
       "1              carlod'este               1991.0           HarperPerennial   \n",
       "2           ginabarikolata               1999.0      Farrar Straus Giroux   \n",
       "3                   amytan               1991.0          Putnam Pub Group   \n",
       "4             robertcowley               2000.0  Berkley Publishing Group   \n",
       "...                    ...                  ...                       ...   \n",
       "149516        p.j.o'rourke               1987.0              Pocket Books   \n",
       "149517         samlightner               2001.0            Broadway Books   \n",
       "149518        claudedooley               1985.0           Lone Star Books   \n",
       "149519         jeremylloyd               1997.0                Kqed Books   \n",
       "149520              mapsco               1999.0  American Map Corporation   \n",
       "\n",
       "                                                  img_url  language  \\\n",
       "0       http://images.amazon.com/images/P/0002005018.0...         1   \n",
       "1       http://images.amazon.com/images/P/0060973129.0...         1   \n",
       "2       http://images.amazon.com/images/P/0374157065.0...         1   \n",
       "3       http://images.amazon.com/images/P/0399135782.0...         1   \n",
       "4       http://images.amazon.com/images/P/0425176428.0...         1   \n",
       "...                                                   ...       ...   \n",
       "149516  http://images.amazon.com/images/P/067161746X.0...         1   \n",
       "149517  http://images.amazon.com/images/P/0767907566.0...         1   \n",
       "149518  http://images.amazon.com/images/P/0884159221.0...         1   \n",
       "149519  http://images.amazon.com/images/P/0912333022.0...         1   \n",
       "149520  http://images.amazon.com/images/P/1569661057.0...         1   \n",
       "\n",
       "           category                                            summary  \\\n",
       "0         actresses  In a small town in Canada, Clara Callan reluct...   \n",
       "1            others  Here, for the first time in paperback, is an o...   \n",
       "2           medical  Describes the great flu epidemic of 1918, an o...   \n",
       "3           fiction  A Chinese immigrant who is convinced she is dy...   \n",
       "4           history  Essays by respected military historians, inclu...   \n",
       "...             ...                                                ...   \n",
       "149516        humor  A tongue-in-cheek survival guide for single pe...   \n",
       "149517       nature  A daring twist on the travel-adventure genre t...   \n",
       "149518  ['Fiction']  Why stop?: A guide to Texas historical roadsid...   \n",
       "149519      fiction  These hilarious stories by the creator of publ...   \n",
       "149520  ['Fiction']  Dallas Street Map Guide and Directory, 2000 Ed...   \n",
       "\n",
       "                                 img_path  \n",
       "0       images/0002005018.01.THUMBZZZ.jpg  \n",
       "1       images/0060973129.01.THUMBZZZ.jpg  \n",
       "2       images/0374157065.01.THUMBZZZ.jpg  \n",
       "3       images/0399135782.01.THUMBZZZ.jpg  \n",
       "4       images/0425176428.01.THUMBZZZ.jpg  \n",
       "...                                   ...  \n",
       "149516  images/067161746X.01.THUMBZZZ.jpg  \n",
       "149517  images/0767907566.01.THUMBZZZ.jpg  \n",
       "149518  images/0884159221.01.THUMBZZZ.jpg  \n",
       "149519  images/0912333022.01.THUMBZZZ.jpg  \n",
       "149520  images/1569661057.01.THUMBZZZ.jpg  \n",
       "\n",
       "[149521 rows x 10 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['books']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data[\u001b[39m'\u001b[39;49m\u001b[39mtext_train\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text_train'"
     ]
    }
   ],
   "source": [
    "data['text_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>rating</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>user_summary_merge_vector</th>\n",
       "      <th>item_summary_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.30769682, 0.0151690515, 0.3217908, -0.1050...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13426</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in a small town in canada clara callan relucta...</td>\n",
       "      <td>107</td>\n",
       "      <td>[-0.20867082, 0.3197794, 0.13658687, -0.140712...</td>\n",
       "      <td>[-0.21812883, -0.20391987, 0.06439817, -0.1359...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26761</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>here for the first time in paperback is an out...</td>\n",
       "      <td>235</td>\n",
       "      <td>[-0.5034143, 0.16244616, 0.0833784, -0.0654003...</td>\n",
       "      <td>[-0.5386576, 0.12224596, 0.078437544, -0.05627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16495</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>describes the great flu epidemic of 1918 an ou...</td>\n",
       "      <td>218</td>\n",
       "      <td>[-0.05029916, 0.130561, -0.17901792, -0.152968...</td>\n",
       "      <td>[-0.05029916, 0.130561, -0.17901792, -0.152968...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6225</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>199</td>\n",
       "      <td>[0.030334461, 0.29610607, 0.40644652, -0.13158...</td>\n",
       "      <td>[0.124538615, 0.2809713, 0.23661214, -0.281009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76694</th>\n",
       "      <td>7728</td>\n",
       "      <td>149565</td>\n",
       "      <td>0</td>\n",
       "      <td>on becoming childwise responds to this need by...</td>\n",
       "      <td>156</td>\n",
       "      <td>[-0.15238021, 0.5389414, -0.10356555, -0.10182...</td>\n",
       "      <td>[-0.15238021, 0.5389414, -0.10356555, -0.10182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76695</th>\n",
       "      <td>47785</td>\n",
       "      <td>149566</td>\n",
       "      <td>0</td>\n",
       "      <td>respektlos macht der autor mit der griechische...</td>\n",
       "      <td>157</td>\n",
       "      <td>[-0.6279584, 0.1575934, 0.19220233, -0.2471607...</td>\n",
       "      <td>[-0.64320105, 0.06612672, 0.114306755, -0.2445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76696</th>\n",
       "      <td>4209</td>\n",
       "      <td>149567</td>\n",
       "      <td>0</td>\n",
       "      <td>the fascinating characters in this short story...</td>\n",
       "      <td>177</td>\n",
       "      <td>[-0.38790068, 0.11098844, 0.27980253, -0.12648...</td>\n",
       "      <td>[0.11831131, 0.26141438, 0.47210327, -0.080064...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76697</th>\n",
       "      <td>40779</td>\n",
       "      <td>149568</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.066250935, -0.11153858, 0.22444469, -0.003...</td>\n",
       "      <td>[-0.07091211, -0.1524299, -0.28767318, -0.0033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76698</th>\n",
       "      <td>1879</td>\n",
       "      <td>149569</td>\n",
       "      <td>0</td>\n",
       "      <td>a daring twist on the travel adventure genre t...</td>\n",
       "      <td>230</td>\n",
       "      <td>[-0.38273704, 0.12612452, 0.4502024, -0.041709...</td>\n",
       "      <td>[-0.3169796, 0.19284458, 0.42234662, -0.121430...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76699 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id    isbn  rating  \\\n",
       "0           13       0       0   \n",
       "1        13426       0       0   \n",
       "2        26761       1       0   \n",
       "3        16495       2       0   \n",
       "4         6225       3       0   \n",
       "...        ...     ...     ...   \n",
       "76694     7728  149565       0   \n",
       "76695    47785  149566       0   \n",
       "76696     4209  149567       0   \n",
       "76697    40779  149568       0   \n",
       "76698     1879  149569       0   \n",
       "\n",
       "                                                 summary  summary_length  \\\n",
       "0      in a small town in canada clara callan relucta...             107   \n",
       "1      in a small town in canada clara callan relucta...             107   \n",
       "2      here for the first time in paperback is an out...             235   \n",
       "3      describes the great flu epidemic of 1918 an ou...             218   \n",
       "4      a chinese immigrant who is convinced she is dy...             199   \n",
       "...                                                  ...             ...   \n",
       "76694  on becoming childwise responds to this need by...             156   \n",
       "76695  respektlos macht der autor mit der griechische...             157   \n",
       "76696  the fascinating characters in this short story...             177   \n",
       "76697                                               none               4   \n",
       "76698  a daring twist on the travel adventure genre t...             230   \n",
       "\n",
       "                               user_summary_merge_vector  \\\n",
       "0      [-0.30769682, 0.0151690515, 0.3217908, -0.1050...   \n",
       "1      [-0.20867082, 0.3197794, 0.13658687, -0.140712...   \n",
       "2      [-0.5034143, 0.16244616, 0.0833784, -0.0654003...   \n",
       "3      [-0.05029916, 0.130561, -0.17901792, -0.152968...   \n",
       "4      [0.030334461, 0.29610607, 0.40644652, -0.13158...   \n",
       "...                                                  ...   \n",
       "76694  [-0.15238021, 0.5389414, -0.10356555, -0.10182...   \n",
       "76695  [-0.6279584, 0.1575934, 0.19220233, -0.2471607...   \n",
       "76696  [-0.38790068, 0.11098844, 0.27980253, -0.12648...   \n",
       "76697  [-0.066250935, -0.11153858, 0.22444469, -0.003...   \n",
       "76698  [-0.38273704, 0.12612452, 0.4502024, -0.041709...   \n",
       "\n",
       "                                     item_summary_vector  \n",
       "0      [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "1      [-0.21812883, -0.20391987, 0.06439817, -0.1359...  \n",
       "2      [-0.5386576, 0.12224596, 0.078437544, -0.05627...  \n",
       "3      [-0.05029916, 0.130561, -0.17901792, -0.152968...  \n",
       "4      [0.124538615, 0.2809713, 0.23661214, -0.281009...  \n",
       "...                                                  ...  \n",
       "76694  [-0.15238021, 0.5389414, -0.10356555, -0.10182...  \n",
       "76695  [-0.64320105, 0.06612672, 0.114306755, -0.2445...  \n",
       "76696  [0.11831131, 0.26141438, 0.47210327, -0.080064...  \n",
       "76697  [-0.07091211, -0.1524299, -0.28767318, -0.0033...  \n",
       "76698  [-0.3169796, 0.19284458, 0.42234662, -0.121430...  \n",
       "\n",
       "[76699 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>user_summary_merge_vector</th>\n",
       "      <th>item_summary_vector</th>\n",
       "      <th>age</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>category</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>book_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305139</th>\n",
       "      <td>53609</td>\n",
       "      <td>128125</td>\n",
       "      <td>[-0.666528, -0.08152228, 0.24727836, -0.084604...</td>\n",
       "      <td>[-0.5255218, -0.18674482, -0.027163552, -0.088...</td>\n",
       "      <td>4</td>\n",
       "      <td>10508</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7761</td>\n",
       "      <td>1</td>\n",
       "      <td>51922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248252</th>\n",
       "      <td>53633</td>\n",
       "      <td>77836</td>\n",
       "      <td>[-0.17015937, 0.22601393, -0.0025442836, 0.031...</td>\n",
       "      <td>[-0.17015937, 0.22601393, -0.0025442836, 0.031...</td>\n",
       "      <td>2</td>\n",
       "      <td>218</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>2341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220753</th>\n",
       "      <td>50354</td>\n",
       "      <td>58530</td>\n",
       "      <td>[-0.48111784, -0.12800646, 0.23085281, -0.3305...</td>\n",
       "      <td>[-0.48111784, -0.12800646, 0.23085281, -0.3305...</td>\n",
       "      <td>3</td>\n",
       "      <td>341</td>\n",
       "      <td>112</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136507</th>\n",
       "      <td>8620</td>\n",
       "      <td>19296</td>\n",
       "      <td>[-0.28503436, 0.11193334, 0.30336663, -0.10924...</td>\n",
       "      <td>[0.08228267, 0.044763505, 0.4291388, -0.042289...</td>\n",
       "      <td>4</td>\n",
       "      <td>176</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253212</th>\n",
       "      <td>38011</td>\n",
       "      <td>81707</td>\n",
       "      <td>[-0.055598613, 0.14226958, 0.14113417, -0.1201...</td>\n",
       "      <td>[-0.017605307, 0.42901292, -0.19937387, -0.078...</td>\n",
       "      <td>2</td>\n",
       "      <td>1751</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>6811</td>\n",
       "      <td>0</td>\n",
       "      <td>32660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>2752</td>\n",
       "      <td>14936</td>\n",
       "      <td>[-0.25754997, 0.043545574, 0.23535174, 0.02233...</td>\n",
       "      <td>[-0.064633794, -0.048247874, 0.1098913, -0.209...</td>\n",
       "      <td>3</td>\n",
       "      <td>1536</td>\n",
       "      <td>230</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>8108</td>\n",
       "      <td>86566</td>\n",
       "      <td>[-0.26142243, -0.022423547, 0.5186753, -0.2043...</td>\n",
       "      <td>[-0.22999044, -0.1659187, -0.15864529, -0.1466...</td>\n",
       "      <td>3</td>\n",
       "      <td>1881</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7060</td>\n",
       "      <td>0</td>\n",
       "      <td>34317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>10534</td>\n",
       "      <td>18168</td>\n",
       "      <td>[-0.32386982, 0.027195103, 0.20032687, -0.1267...</td>\n",
       "      <td>[0.27944458, 0.06647224, 0.08743789, 0.0796389...</td>\n",
       "      <td>5</td>\n",
       "      <td>3716</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>40246</td>\n",
       "      <td>22419</td>\n",
       "      <td>[-0.4072631, 0.1315497, 0.13957448, -0.2512652...</td>\n",
       "      <td>[-0.4072631, 0.1315497, 0.13957448, -0.2512652...</td>\n",
       "      <td>2</td>\n",
       "      <td>8590</td>\n",
       "      <td>1034</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>24097</td>\n",
       "      <td>15452</td>\n",
       "      <td>[-0.34905925, -0.14139089, 0.21134925, -0.1269...</td>\n",
       "      <td>[0.008132203, -0.24694961, 0.36239615, -0.4082...</td>\n",
       "      <td>1</td>\n",
       "      <td>5121</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1889</td>\n",
       "      <td>0</td>\n",
       "      <td>7197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>291455 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    isbn                          user_summary_merge_vector  \\\n",
       "305139    53609  128125  [-0.666528, -0.08152228, 0.24727836, -0.084604...   \n",
       "248252    53633   77836  [-0.17015937, 0.22601393, -0.0025442836, 0.031...   \n",
       "220753    50354   58530  [-0.48111784, -0.12800646, 0.23085281, -0.3305...   \n",
       "136507     8620   19296  [-0.28503436, 0.11193334, 0.30336663, -0.10924...   \n",
       "253212    38011   81707  [-0.055598613, 0.14226958, 0.14113417, -0.1201...   \n",
       "...         ...     ...                                                ...   \n",
       "119879     2752   14936  [-0.25754997, 0.043545574, 0.23535174, 0.02233...   \n",
       "259178     8108   86566  [-0.26142243, -0.022423547, 0.5186753, -0.2043...   \n",
       "131932    10534   18168  [-0.32386982, 0.027195103, 0.20032687, -0.1267...   \n",
       "146867    40246   22419  [-0.4072631, 0.1315497, 0.13957448, -0.2512652...   \n",
       "121958    24097   15452  [-0.34905925, -0.14139089, 0.21134925, -0.1269...   \n",
       "\n",
       "                                      item_summary_vector  age  location_city  \\\n",
       "305139  [-0.5255218, -0.18674482, -0.027163552, -0.088...    4          10508   \n",
       "248252  [-0.17015937, 0.22601393, -0.0025442836, 0.031...    2            218   \n",
       "220753  [-0.48111784, -0.12800646, 0.23085281, -0.3305...    3            341   \n",
       "136507  [0.08228267, 0.044763505, 0.4291388, -0.042289...    4            176   \n",
       "253212  [-0.017605307, 0.42901292, -0.19937387, -0.078...    2           1751   \n",
       "...                                                   ...  ...            ...   \n",
       "119879  [-0.064633794, -0.048247874, 0.1098913, -0.209...    3           1536   \n",
       "259178  [-0.22999044, -0.1659187, -0.15864529, -0.1466...    3           1881   \n",
       "131932  [0.27944458, 0.06647224, 0.08743789, 0.0796389...    5           3716   \n",
       "146867  [-0.4072631, 0.1315497, 0.13957448, -0.2512652...    2           8590   \n",
       "121958  [0.008132203, -0.24694961, 0.36239615, -0.4082...    1           5121   \n",
       "\n",
       "        location_state  location_country  category  publisher  language  \\\n",
       "305139              46                 7         5       7761         1   \n",
       "248252               4                 1         5        232         0   \n",
       "220753             112                 7         5         27         1   \n",
       "136507              64                 0         3         65         0   \n",
       "253212              30                 1        11       6811         0   \n",
       "...                ...               ...       ...        ...       ...   \n",
       "119879             230                 1         3         21         0   \n",
       "259178              23                 1         5       7060         0   \n",
       "131932              40                 1         5         21         0   \n",
       "146867            1034                12         3         41         1   \n",
       "121958              22                 1         5       1889         0   \n",
       "\n",
       "        book_author  \n",
       "305139        51922  \n",
       "248252         2341  \n",
       "220753         4500  \n",
       "136507          273  \n",
       "253212        32660  \n",
       "...             ...  \n",
       "119879         1261  \n",
       "259178        34317  \n",
       "131932          550  \n",
       "146867          157  \n",
       "121958         7197  \n",
       "\n",
       "[291455 rows x 12 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['X_train'] # split 할때 shuffle 해서 index가 뒤죽박죽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245436\n",
      "61359\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(data['X_train']))\n",
    "print(len(data['X_valid']))\n",
    "print(len(data['X_train']) + len(data['X_valid']) == len(data['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Dataset(Dataset):\n",
    "    def __init__(self, user_isbn_vector, user_summary_merge_vector, item_summary_vector, cat, img_vector, label):\n",
    "        print(user_isbn_vector[:, 0].shape)\n",
    "        self.user_isbn_vector = user_isbn_vector\n",
    "        self.user_summary_merge_vector = user_summary_merge_vector\n",
    "        self.item_summary_vector = item_summary_vector\n",
    "        self.cat = cat\n",
    "        self.img_vector = img_vector\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_isbn_vector.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "                'user_isbn_vector' : torch.tensor(self.user_isbn_vector[i], dtype=torch.long),\n",
    "                'user_summary_merge_vector' : torch.tensor(self.user_summary_merge_vector[i].reshape(-1, 1), dtype=torch.float32),\n",
    "                'item_summary_vector' : torch.tensor(self.item_summary_vector[i].reshape(-1, 1), dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label[i], dtype=torch.float32),\n",
    "                'cat' : torch.LongTensor(self.cat[i]),\n",
    "                'img_vector' : torch.tensor(self.img_vector[i], dtype=torch.float32),\n",
    "                'user_id': self.user_isbn_vector[i, 0],\n",
    "                'isbn': self.user_isbn_vector[i, 1],\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245436,)\n",
      "(61359,)\n",
      "(76699,)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Text_Dataset(\n",
    "                            data['X_train'][['user_id', 'isbn']].values, # idx\n",
    "                            data['X_train']['user_summary_merge_vector'].values,\n",
    "                            data['X_train']['item_summary_vector'].values,\n",
    "                            data['X_train'][['user_id', 'isbn', 'age', 'location_city', 'location_state',\n",
    "       'location_country', 'category', 'publisher', 'language', 'book_author']].values,\n",
    "                            data['X_train']['img_vector'].values,\n",
    "                            data['y_train'].values,\n",
    "                            )\n",
    "valid_dataset = Text_Dataset(\n",
    "                            data['X_valid'][['user_id', 'isbn']].values, # idx\n",
    "                            data['X_valid']['user_summary_merge_vector'].values,\n",
    "                            data['X_valid']['item_summary_vector'].values,\n",
    "                            data['X_valid'][['user_id', 'isbn', 'age', 'location_city', 'location_state',\n",
    "       'location_country', 'category', 'publisher', 'language', 'book_author']].values,\n",
    "                            data['X_valid']['img_vector'].values,\n",
    "                            data['y_valid'].values,\n",
    "                            )\n",
    "test_dataset = Text_Dataset(\n",
    "                            data['test'][['user_id', 'isbn']].values, # idx\n",
    "                            data['test']['user_summary_merge_vector'].values,\n",
    "                            data['test']['item_summary_vector'].values,\n",
    "                            data['test'][['user_id', 'isbn', 'age', 'location_city', 'location_state',\n",
    "       'location_country', 'category', 'publisher', 'language', 'book_author']].values,\n",
    "                            data['test']['img_vector'].values,\n",
    "                            data['test']['rating'].values,\n",
    "                            )\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args.BATCH_SIZE, num_workers=8, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.BATCH_SIZE, num_workers=8, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args.BATCH_SIZE, num_workers=8, shuffle=False)\n",
    "data['train_dataloader'], data['valid_dataloader'], data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245436"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['user_isbn_vector', 'user_summary_merge_vector', 'item_summary_vector', 'label', 'cat', 'img_vector', 'user_id', 'isbn'])\n",
      "\n",
      "user_isbn_vector\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1024, 2])\n",
      "tensor([[ 3697,  4711],\n",
      "        [ 1394,   728],\n",
      "        [33583, 43254],\n",
      "        [17480, 15247],\n",
      "        [12936, 97103]])\n",
      "\n",
      "user_summary_merge_vector\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1024, 768, 1])\n",
      "\n",
      "item_summary_vector\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1024, 768, 1])\n",
      "\n",
      "label\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1024])\n",
      "tensor([7., 5., 3., 9., 2.])\n",
      "\n",
      "user_id\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1024])\n",
      "tensor([ 3697,  1394, 33583, 17480, 12936])\n",
      "\n",
      "isbn\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1024])\n",
      "tensor([ 4711,   728, 43254, 15247, 97103])\n"
     ]
    }
   ],
   "source": [
    "next_iter_train_dataloader = next(iter(train_dataloader))\n",
    "print(next_iter_train_dataloader.keys())\n",
    "print()\n",
    "print('user_isbn_vector')\n",
    "print(type(next_iter_train_dataloader['user_isbn_vector'])) # id. sub도 id임.. 굳이 user, isbn으로 바꿔야 할까? 나중에 보면 알 듯. \n",
    "print(next_iter_train_dataloader['user_isbn_vector'].shape)\n",
    "print(next_iter_train_dataloader['user_isbn_vector'][:5])\n",
    "print()\n",
    "print('user_summary_merge_vector')\n",
    "print(type(next_iter_train_dataloader['user_summary_merge_vector']))\n",
    "print(next_iter_train_dataloader['user_summary_merge_vector'].shape)\n",
    "print()\n",
    "print('item_summary_vector')\n",
    "print(type(next_iter_train_dataloader['item_summary_vector']))\n",
    "print(next_iter_train_dataloader['item_summary_vector'].shape)\n",
    "print()\n",
    "print('label')\n",
    "print(type(next_iter_train_dataloader['label']))\n",
    "print(next_iter_train_dataloader['label'].shape)\n",
    "print(next_iter_train_dataloader['label'][:5])\n",
    "print()\n",
    "print('user_id')\n",
    "print(type(next_iter_train_dataloader['user_id']))\n",
    "print(next_iter_train_dataloader['user_id'].shape)\n",
    "print(next_iter_train_dataloader['user_id'][:5]) # id\n",
    "print()\n",
    "print('isbn')\n",
    "print(type(next_iter_train_dataloader['isbn']))\n",
    "print(next_iter_train_dataloader['isbn'].shape)\n",
    "print(next_iter_train_dataloader['isbn'][:5]) # id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "isbn\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1024])\n",
      "tensor([ 4711,   728, 43254, 15247, 97103])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print()\n",
    "print('isbn')\n",
    "print(type(next_iter_train_dataloader['isbn']))\n",
    "print(next_iter_train_dataloader['isbn'].shape)\n",
    "print(next_iter_train_dataloader['isbn'][:5]) # id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'field_dims', 'users', 'books', 'sub', 'idx2user', 'idx2isbn', 'user2idx', 'isbn2idx', 'img_train', 'img_test', 'X_train', 'X_valid', 'y_train', 'y_valid', 'train_dataloader', 'valid_dataloader', 'test_dataloader'])"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "def rmse(real: list, predict: list) -> float:\n",
    "    pred = np.array(predict)\n",
    "    return np.sqrt(np.mean((real-pred) ** 2))\n",
    "\n",
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y)+self.eps)\n",
    "        return loss\n",
    "\n",
    "class FeaturesEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class FactorizationMachine_v(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.v = nn.Parameter(torch.rand(input_dim, latent_dim), requires_grad = True)\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = self.linear(x)\n",
    "        square_of_sum = torch.mm(x, self.v) ** 2\n",
    "        sum_of_square = torch.mm(x ** 2, self.v ** 2)\n",
    "        pair_interactions = torch.sum(square_of_sum - sum_of_square, dim=1, keepdim=True)\n",
    "        output = linear + (0.5 * pair_interactions)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    \"\"\"The Residual block of ResNet.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv1d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv1d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv1d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm1d()\n",
    "        self.bn2 = nn.LazyBatchNorm1d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, word_dim, out_dim, kernel_size, conv_1d_out_dim):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                                Residual(word_dim, True),\n",
    "                                nn.MaxPool2d(kernel_size=(2, 1)),\n",
    "                                nn.Dropout(p=0.4),\n",
    "                                Residual(word_dim//2, True),\n",
    "                                nn.MaxPool2d(kernel_size=(2, 1)),\n",
    "                                nn.Dropout(p=0.4),\n",
    "                                Residual(word_dim//4, True),\n",
    "                                nn.MaxPool2d(kernel_size=(2, 1)),\n",
    "                                nn.Dropout(p=0.4),\n",
    "                                Residual(word_dim//8, True),\n",
    "                                nn.MaxPool2d(kernel_size=(2, 1)),\n",
    "                                nn.Dropout(p=0.4),\n",
    "                                Residual(word_dim//16, True),\n",
    "                                nn.MaxPool2d(kernel_size=(2, 1)),\n",
    "                                nn.Dropout(p=0.4),\n",
    "                                Residual(word_dim//32, True),\n",
    "                                nn.Dropout(p=0.4),\n",
    "                                nn.AdaptiveAvgPool1d(1),\n",
    "                                )\n",
    "        self.bn1 = nn.LazyBatchNorm1d()\n",
    "        self.residual_dropout = nn.Dropout(0.4)\n",
    "        self.linear1 = nn.Linear(word_dim//32, word_dim//64) # out_dim, cnn_conv_output도... 이 필요가 없어짐... 24 -> 24,,\n",
    "        self.linear2 = nn.Linear(word_dim//64, word_dim//32)\n",
    "\n",
    "    def forward(self, vec):\n",
    "        output = self.conv(vec).squeeze(-1)\n",
    "        \n",
    "        output_ = self.linear1(output.view(-1, output.size(-1))) # 16차원이 되어 나옴\n",
    "        output_ = F.relu(self.bn1(output_))\n",
    "        output_ = self.linear2(output_) \n",
    "        # print(output.shape, output_.shape)\n",
    "        # print(self.residual_dropout(output).shape)\n",
    "        output = output_ + self.residual_dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class Residual2(nn.Module):\n",
    "    \"\"\"The Residual block of ResNet.\"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        # print(Y.shape, X.shape)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "class CNN_Base(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(CNN_Base, self).__init__()\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "            Residual2(3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=1),\n",
    "            Residual2(16),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1, stride=1),\n",
    "            Residual2(32),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layer(x)\n",
    "        x = x.view(-1, 32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DeepCoNN(nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, word_dim, out_dim, kernel_size, conv_1d_out_dim, latent_dim):\n",
    "        super(_DeepCoNN, self).__init__()\n",
    "        self.embedding = FeaturesEmbedding(field_dims, embed_dim)\n",
    "        self.cnn_u = CNN_1D(\n",
    "                             word_dim=word_dim,\n",
    "                             out_dim=out_dim,\n",
    "                             kernel_size=kernel_size,\n",
    "                             conv_1d_out_dim=conv_1d_out_dim,\n",
    "                            )\n",
    "        self.cnn_i = CNN_1D(\n",
    "                             word_dim=word_dim,\n",
    "                             out_dim=out_dim,\n",
    "                             kernel_size=kernel_size,\n",
    "                             conv_1d_out_dim=conv_1d_out_dim,\n",
    "                            )\n",
    "        self.fm = FactorizationMachine_v(\n",
    "                                         input_dim=(conv_1d_out_dim * 2) + (embed_dim*len(field_dims)),\n",
    "                                         latent_dim=latent_dim,\n",
    "                                         )\n",
    "    def forward(self, x):\n",
    "        user_isbn_vector, user_text_vector, item_text_vector = x[0], x[1], x[2]\n",
    "        user_isbn_feature = self.embedding(user_isbn_vector)\n",
    "        user_text_feature = self.cnn_u(user_text_vector)\n",
    "        item_text_feature = self.cnn_i(item_text_vector)\n",
    "        # print(user_isbn_feature.shape)\n",
    "        # print(user_text_feature.shape)\n",
    "        # print(item_text_feature.shape)\n",
    "\n",
    "        feature_vector = torch.cat([\n",
    "                                    user_isbn_feature.view(-1, user_isbn_feature.size(1) * user_isbn_feature.size(2)), # 8, 8\n",
    "                                    user_text_feature, # 16\n",
    "                                    item_text_feature # 16\n",
    "                                    ], dim=1)\n",
    "        # print(f'ffm_feature_vector: {feature_vector.shape}')\n",
    "        # output = self.fm(feature_vector)\n",
    "        # return output.squeeze(1)\n",
    "        return feature_vector\n",
    "\n",
    "class FieldAwareFactorizationMachine(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = torch.nn.ModuleList([\n",
    "            torch.nn.Embedding(sum(field_dims), embed_dim) for _ in range(self.num_fields)\n",
    "        ])\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)\n",
    "        for embedding in self.embeddings:\n",
    "            torch.nn.init.xavier_uniform_(embedding.weight.data)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets, dtype=torch.int64).unsqueeze(0)\n",
    "        xs = [self.embeddings[i](x) for i in range(self.num_fields)]\n",
    "        ix = list()\n",
    "        for i in range(self.num_fields - 1):\n",
    "            for j in range(i + 1, self.num_fields):\n",
    "                ix.append(xs[j][:, i] * xs[i][:, j])\n",
    "        ix = torch.stack(ix, dim=1)\n",
    "        return ix\n",
    "\n",
    "class FeaturesLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims: np.ndarray, output_dim: int=1):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n",
    "        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.int64)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n",
    "        return torch.sum(self.fc(x), dim=1) + self.bias\n",
    "\n",
    "class _FieldAwareFactorizationMachineModel(nn.Module):\n",
    "\n",
    "    def __init__(self, field_dims: np.ndarray, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.linear = FeaturesLinear(field_dims, output_dim=16)\n",
    "        self.ffm = FieldAwareFactorizationMachine(field_dims, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        :param x: Long tensor of size ``(batch_size, num_fields)``\n",
    "        \"\"\"\n",
    "        # ffm_term = torch.sum(torch.sum(self.ffm(x), dim=1), dim=1, keepdim=True)\n",
    "        ffm_term = torch.sum(self.ffm(x), dim=1)\n",
    "        # print(f'ffm_term: {ffm_term.shape}, self.linear(x): {self.linear(x).shape}')\n",
    "        # x = self.linear(x) + ffm_term\n",
    "        # # return torch.sigmoid(x.squeeze(1))\n",
    "        # return x.squeeze(1)\n",
    "\n",
    "        x = torch.cat([ffm_term, self.linear(x)], axis=-1)\n",
    "        return x\n",
    "\n",
    "class _MIX(nn.Module):\n",
    "    def __init__(self, args, data):\n",
    "        super().__init__()\n",
    "        self.device = args.DEVICE\n",
    "        self.field_dims = data['field_dims']\n",
    "        self.embed_dim = args.FFM_EMBED_DIM\n",
    "        self.deep_conn = _DeepCoNN(\n",
    "                                np.array([len(data['user2idx']), len(data['isbn2idx'])], dtype=np.uint32),\n",
    "                                args.DEEPCONN_EMBED_DIM,\n",
    "                                args.DEEPCONN_WORD_DIM,\n",
    "                                args.DEEPCONN_OUT_DIM,\n",
    "                                args.DEEPCONN_KERNEL_SIZE,\n",
    "                                args.DEEPCONN_CONV_1D_OUT_DIM,\n",
    "                                args.DEEPCONN_LATENT_DIM\n",
    "                                ).to(self.device)\n",
    "        self.ffm = _FieldAwareFactorizationMachineModel(self.field_dims, self.embed_dim).to(self.device)\n",
    "        self.cnn = CNN_Base()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(96)\n",
    "\n",
    "        self.residual_dropout = nn.Dropout(0.4)\n",
    "        self.fclayer1 = nn.Sequential(\n",
    "            nn.Linear(96, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 96)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fclayer2 = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(96, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, deep_conn_d, ffm_d):\n",
    "        x1 = self.deep_conn(deep_conn_d)\n",
    "        x2 = self.ffm(ffm_d)\n",
    "        # img_feature = self.cnn(img_vector)\n",
    "        # print(img_feature.shape)\n",
    "        # print(f'x1.shape: {x1.shape}, x2.shape: {x2.shape}')\n",
    "        # x = torch.cat([x1, x2, img_feature], axis=-1)\n",
    "        x = torch.cat([x1, x2], axis=-1)\n",
    "\n",
    "        x_ = self.norm_1(x)\n",
    "        x_ = self.fclayer1(x_)\n",
    "        x = x_ + self.residual_dropout(x)\n",
    "\n",
    "        # print(x.shape)\n",
    "        return self.fclayer2(x).squeeze(-1)\n",
    "\n",
    "\n",
    "class MIX:\n",
    "    def __init__(self, args, data):\n",
    "        super(MIX, self).__init__()\n",
    "        self.data = data\n",
    "        self.device = args.DEVICE\n",
    "        self.model = _MIX(args, data)\n",
    "        \n",
    "        self.optimizer =  torch.optim.Adam(self.model.parameters(), lr=args.LR, weight_decay=args.WEIGHT_DECAY) # WD 추가\n",
    "        self.train_data_loader = data['train_dataloader']\n",
    "        self.valid_data_loader = data['valid_dataloader']\n",
    "        self.criterion = RMSELoss()\n",
    "        self.epochs = args.EPOCHS\n",
    "        self.model_name = 'MIX_model'\n",
    "\n",
    "\n",
    "    def train(self, train_user: set, r_avg: dict):\n",
    "        self.model.to(self.device)\n",
    "        writer = SummaryWriter(log_dir=f'/opt/ml/input/exper_logs3/'+ ' '.join(args for args in sys.argv[1:]))\n",
    "        minimum_loss = 999999999\n",
    "        patience_limit = 7\n",
    "        patience_check = 0\n",
    "        loss_list = []\n",
    "        isbn2booksinx = {isbn:idx for idx, isbn in enumerate(self.data['books']['isbn'])}\n",
    "        tk0 = tqdm.tqdm(range(self.epochs), smoothing=0, mininterval=1.0, leave=True)\n",
    "        for epoch in tk0:\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            n = 0\n",
    "            for i, data in enumerate(self.train_data_loader):\n",
    "                deep_conn_d = [data['user_isbn_vector'].to(self.device), data['user_summary_merge_vector'].to(self.device), data['item_summary_vector'].to(self.device)]\n",
    "                fmm_d = data['cat'].to(self.device)\n",
    "                target = data['label'].to(self.device)\n",
    "                # img_vector = data['img_vector'].to(self.device)\n",
    "                y = self.model(deep_conn_d, fmm_d)\n",
    "                loss = self.criterion(y, target.float())\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += (loss.item() ** 2) * len(data['user_isbn_vector'])\n",
    "                n += 1\n",
    "            train_epoch_loss = np.sqrt(total_loss/len(self.train_data_loader.dataset))\n",
    "            writer.add_scalar(\"loss/train_epoch\", train_epoch_loss, epoch+1)\n",
    "            self.model.eval()\n",
    "            val_total_loss = 0\n",
    "            val_n = 0\n",
    "\n",
    "            cold_count = 0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(self.valid_data_loader):\n",
    "                    deep_conn_d = [data['user_isbn_vector'].to(self.device), data['user_summary_merge_vector'].to(self.device), data['item_summary_vector'].to(self.device)]\n",
    "                    fmm_d = data['cat'].to(self.device)\n",
    "                    target = data['label'].to(self.device)\n",
    "                    # img_vector = data['img_vector'].to(self.device)\n",
    "                    y = self.model(deep_conn_d, fmm_d)\n",
    "\n",
    "                    loss = self.criterion(y, target.float())\n",
    "                    val_total_loss += (loss.item() ** 2) * len(data['user_isbn_vector'])\n",
    "                    val_n += 1\n",
    "            print(cold_count, len(self.valid_data_loader.dataset))\n",
    "            valid_epoch_loss = np.sqrt(val_total_loss/len(self.valid_data_loader.dataset))\n",
    "            writer.add_scalar(\"loss/valid_epoch\", valid_epoch_loss, epoch+1) ## 전부 이걸로 바꾸기\n",
    "            if minimum_loss > valid_epoch_loss:\n",
    "                patience_check = 0\n",
    "                minimum_loss = valid_epoch_loss\n",
    "                if not os.path.exists('./models'):\n",
    "                    os.makedirs('./models')\n",
    "                torch.save(self.model.state_dict(), './models/{}.pt'.format(self.model_name))\n",
    "                loss_list.append([epoch, train_epoch_loss, valid_epoch_loss, 'Model saved'])\n",
    "                print('updated!')\n",
    "            else:\n",
    "                patience_check += 1\n",
    "                loss_list.append([epoch, train_epoch_loss, valid_epoch_loss, 'None'])\n",
    "                if patience_check >= patience_limit:\n",
    "                    break\n",
    "                print(f'patience_check: {patience_check}')\n",
    "            tk0.set_postfix(train_loss=train_epoch_loss, valid_loss=valid_epoch_loss)\n",
    "            print(train_epoch_loss, valid_epoch_loss)\n",
    "        writer.add_scalar(\"minimum_loss\", minimum_loss, 0)\n",
    "        writer.close()\n",
    "        return minimum_loss\n",
    "\n",
    "\n",
    "    def predict(self, test_data_loader, train_user, r_avg):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        self.model.load_state_dict(torch.load('./models/{}.pt'.format(self.model_name)))\n",
    "        targets, predicts = list(), list()\n",
    "\n",
    "        cold_count = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_data_loader:\n",
    "                deep_conn_d = [data['user_isbn_vector'].to(self.device), data['user_summary_merge_vector'].to(self.device), data['item_summary_vector'].to(self.device)]\n",
    "                fmm_d = data['cat'].to(self.device)\n",
    "                target = data['label'].to(self.device)\n",
    "                img_vector = data['img_vector'].to(self.device)\n",
    "                y = self.model(deep_conn_d, fmm_d, img_vector)\n",
    "\n",
    "                # for i, user_id in enumerate(data['user_id']): # 배치사이즈만큼의 user_id_idx\n",
    "                #     if user_id.item() not in train_user: # cold user\n",
    "                #         if self.data['books'][self.data['books']['isbn'] == self.data['idx2isbn'][data['isbn'][i].item()]]['summary'] != 'None':\n",
    "                #             cold_count+=1\n",
    "                #             if self.data['idx2isbn'][data['isbn'][i].item()] in r_avg.keys():\n",
    "                #                 y[i] = r_avg[self.data['idx2isbn'][data['isbn'][i].item()]] # isbn -> avg_rating\n",
    "                #             else:\n",
    "                #                 y[i] = 7.069\n",
    "\n",
    "                targets.extend(target.tolist())\n",
    "                predicts.extend(y.tolist())\n",
    "        print(cold_count, len(test_data_loader.dataset))\n",
    "        return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = MIX(args, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DATA_PATH='/opt/ml/input/code/data/', MODEL=None, DATA_SHUFFLE=True, TEST_SIZE=0.2, SEED=42, BATCH_SIZE=1024, EPOCHS=50, LR=0.0007, WEIGHT_DECAY=0.004, DEVICE='cuda', FM_EMBED_DIM=16, FFM_EMBED_DIM=16, NCF_EMBED_DIM=16, NCF_MLP_DIMS=(16, 16), NCF_DROPOUT=0.2, WDN_EMBED_DIM=16, WDN_MLP_DIMS=(16, 16), WDN_DROPOUT=0.2, DCN_EMBED_DIM=16, DCN_MLP_DIMS=(16, 16), DCN_DROPOUT=0.2, DCN_NUM_LAYERS=3, CNN_FM_EMBED_DIM=128, CNN_FM_LATENT_DIM=8, DEEPCONN_VECTOR_CREATE=False, DEEPCONN_EMBED_DIM=8, DEEPCONN_LATENT_DIM=8, DEEPCONN_CONV_1D_OUT_DIM=16, DEEPCONN_KERNEL_SIZE=3, DEEPCONN_WORD_DIM=768, DEEPCONN_OUT_DIM=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [01:01<50:12, 61.47s/it, train_loss=3.46, valid_loss=2.41]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "3.456294348123309 2.4107808803007615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: \n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [02:03<49:13, 61.53s/it, train_loss=2.51, valid_loss=2.3]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.510598352677241 2.296677922755681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [03:05<48:30, 61.92s/it, train_loss=2.39, valid_loss=2.24]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.392713819711575 2.2442873038532962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    \n",
      "if w.is_alive():  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    \n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [04:07<47:26, 61.87s/it, train_loss=2.32, valid_loss=2.22]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.324564979247737 2.223920422469329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [05:09<46:23, 61.85s/it, train_loss=2.27, valid_loss=2.21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.267939416438516 2.206578179546661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      " 12%|█▏        | 6/50 [06:09<45:09, 61.58s/it, train_loss=2.2, valid_loss=2.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 1\n",
      "2.2044793665632354 2.2107627346411456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      " 14%|█▍        | 7/50 [07:09<43:58, 61.36s/it, train_loss=2.13, valid_loss=2.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 2\n",
      "2.1254090973316617 2.216056856487398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      " 16%|█▌        | 8/50 [08:09<42:51, 61.22s/it, train_loss=2.02, valid_loss=2.25]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 3\n",
      "2.0184247601116865 2.2487787441917417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      " 18%|█▊        | 9/50 [09:09<41:45, 61.10s/it, train_loss=1.91, valid_loss=2.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 4\n",
      "1.9143592295192107 2.2830643973313434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      " 20%|██        | 10/50 [10:10<40:40, 61.01s/it, train_loss=1.84, valid_loss=2.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 5\n",
      "1.8435475517130804 2.316113120008549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError\n",
      ": can only test a child processException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      " 22%|██▏       | 11/50 [11:10<39:35, 60.92s/it, train_loss=1.78, valid_loss=2.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 6\n",
      "1.7835906645113655 2.3306904012069545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fc836dfd750>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1493, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/opt/conda/envs/dongyoung/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      " 22%|██▏       | 11/50 [12:10<43:09, 66.40s/it, train_loss=1.78, valid_loss=2.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "minimum_loss = model.train(train_user, r_avg)\n",
    "# residual 추가\n",
    "# image 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts  = model.predict(data['test_dataloader'], train_user, r_avg)\n",
    "submission = pd.read_csv(args.DATA_PATH + 'sample_submission.csv')\n",
    "submission['rating'] = predicts\n",
    "args.MODEL = 'MIX'\n",
    "submission.to_csv('/opt/ml/input/code/submit/{}_{}.csv'.format('_'.join([f'{args.MODEL}', f'{args.BATCH_SIZE}', f'{args.LR}', f'{args.WEIGHT_DECAY}']), minimum_loss), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DATA_PATH='/opt/ml/input/code/data/', MODEL=None, DATA_SHUFFLE=True, TEST_SIZE=0.2, SEED=42, BATCH_SIZE=128, EPOCHS=50, LR=0.0005, WEIGHT_DECAY=0.0008, DEVICE='cuda', FM_EMBED_DIM=16, FFM_EMBED_DIM=16, NCF_EMBED_DIM=16, NCF_MLP_DIMS=(16, 16), NCF_DROPOUT=0.2, WDN_EMBED_DIM=16, WDN_MLP_DIMS=(16, 16), WDN_DROPOUT=0.2, DCN_EMBED_DIM=16, DCN_MLP_DIMS=(16, 16), DCN_DROPOUT=0.2, DCN_NUM_LAYERS=3, CNN_FM_EMBED_DIM=128, CNN_FM_LATENT_DIM=8, DEEPCONN_VECTOR_CREATE=False, DEEPCONN_EMBED_DIM=8, DEEPCONN_LATENT_DIM=8, DEEPCONN_CONV_1D_OUT_DIM=16, DEEPCONN_KERNEL_SIZE=3, DEEPCONN_WORD_DIM=768, DEEPCONN_OUT_DIM=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [02:11<1:47:29, 131.62s/it, train_loss=2.82, valid_loss=2.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.8227574460523033 2.3912808730383914\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [04:23<1:45:34, 131.97s/it, train_loss=2.38, valid_loss=2.22]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.3810383631280336 2.220413473148122\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [06:35<1:43:22, 131.97s/it, train_loss=2.3, valid_loss=2.19] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.29808215448931 2.1916445515267244\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [08:47<1:41:07, 131.91s/it, train_loss=2.25, valid_loss=2.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.2464036265162783 2.1761429258409777\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [11:00<1:39:00, 132.02s/it, train_loss=2.21, valid_loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.210285830413377 2.1743197541866746\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [13:12<1:36:50, 132.06s/it, train_loss=2.18, valid_loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.184168463739466 2.166407783845751\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [15:22<1:34:26, 131.78s/it, train_loss=2.16, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.157160575667934 2.1619837158658344\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [17:35<1:32:19, 131.89s/it, train_loss=2.14, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.1371007188450792 2.1602207246477687\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [19:46<1:30:03, 131.78s/it, train_loss=2.12, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.118508352022062 2.156791658761517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [21:56<1:27:45, 131.63s/it, train_loss=2.1, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 1\n",
      "2.1012923980951874 2.1587123629877003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [24:07<1:25:32, 131.60s/it, train_loss=2.09, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 2\n",
      "2.086470481735547 2.1571313709089877\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [26:18<1:23:18, 131.54s/it, train_loss=2.08, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.077490231154542 2.155825779651997\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [28:29<1:21:06, 131.52s/it, train_loss=2.06, valid_loss=2.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "2.0648063151723295 2.154307274414532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [30:39<1:18:49, 131.37s/it, train_loss=2.06, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 1\n",
      "2.0552149501675223 2.1553790644512385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [32:49<1:16:34, 131.28s/it, train_loss=2.04, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 2\n",
      "2.0427521872596692 2.1574830811170327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [34:59<1:14:21, 131.21s/it, train_loss=2.03, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 3\n",
      "2.0343841827988443 2.157935021605698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [37:11<1:12:11, 131.26s/it, train_loss=2.03, valid_loss=2.15]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 4\n",
      "2.0258521315380515 2.154782983899439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [39:23<1:10:01, 131.31s/it, train_loss=2.02, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 5\n",
      "2.0186240270574247 2.1584879240825727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [41:34<1:07:49, 131.28s/it, train_loss=2.02, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 6\n",
      "2.0152205727399806 2.1573358934752833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [43:43<1:11:20, 138.08s/it, train_loss=2.02, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(args)\n",
    "minimum_loss = model.train(train_user, r_avg)\n",
    "# BATCH_SIZE=128, LR=0.0005, WEIGHT_DECAY=0.0008\n",
    "# residual 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [03:12<1:32:58, 192.35s/it, train_loss=2.39, valid_loss=2.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [06:21<1:29:05, 190.90s/it, train_loss=2.17, valid_loss=2.2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [09:38<1:26:42, 192.69s/it, train_loss=2.17, valid_loss=2.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [12:51<1:23:36, 192.95s/it, train_loss=2.16, valid_loss=2.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [16:05<1:20:27, 193.10s/it, train_loss=2.16, valid_loss=2.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [19:14<1:16:58, 192.43s/it, train_loss=2.15, valid_loss=2.19]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [22:25<1:13:42, 192.28s/it, train_loss=2.14, valid_loss=2.18]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [25:38<1:10:31, 192.34s/it, train_loss=2.13, valid_loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [28:50<1:07:17, 192.26s/it, train_loss=2.12, valid_loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [31:57<1:03:54, 191.75s/it, train_loss=2.12, valid_loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 1\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [35:05<1:00:37, 191.44s/it, train_loss=2.11, valid_loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [38:20<57:30, 191.71s/it, train_loss=2.11, valid_loss=2.17]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [41:37<54:25, 192.11s/it, train_loss=2.1, valid_loss=2.17]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [44:52<51:16, 192.30s/it, train_loss=2.09, valid_loss=2.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [48:03<48:03, 192.22s/it, train_loss=2.09, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [51:12<44:48, 192.03s/it, train_loss=2.08, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [54:22<41:34, 191.89s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [57:34<38:23, 191.94s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [1:00:41<35:08, 191.64s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 1\n",
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [1:03:48<31:54, 191.40s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [1:06:58<28:42, 191.37s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [1:10:06<25:29, 191.22s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [1:13:15<22:17, 191.10s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [1:16:21<19:05, 190.91s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [1:19:28<15:53, 190.75s/it, train_loss=2.06, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [1:22:40<12:43, 190.78s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [1:25:48<13:12, 198.02s/it, train_loss=2.07, valid_loss=2.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "minimum_loss = model.train(train_user, r_avg) # MIX_32_0.001_0.0004_2.1580838039474832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:26<12:50, 26.56s/it, train_loss=16.8, valid_loss=3.09]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:54<12:36, 27.00s/it, train_loss=2.48, valid_loss=2.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:20<12:03, 26.79s/it, train_loss=1.92, valid_loss=2.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "patience_check: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:25<12:50, 28.54s/it, train_loss=1.92, valid_loss=2.35]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m minimum_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(train_user, r_avg)\n",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 30\u001b[0m in \u001b[0;36mDeepCoNN.train\u001b[0;34m(self, train_user, r_avg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39muser_isbn_vector\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X55sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/adam.py:305\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    303\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[1;32m    307\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "minimum_loss = model.train(train_user, r_avg) # user도 없고, book summary가 none일 때만. 그때 book rating이 있으면 그걸로, 없으면 7.069로 -> predict랑 더하고 avg랑 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:29<14:06, 29.19s/it, train_loss=19.1, valid_loss=2.84]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:55<12:59, 27.85s/it, train_loss=2.49, valid_loss=2.37]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:22<12:19, 27.37s/it, train_loss=1.94, valid_loss=2.39]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "patience_check: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:48<11:45, 27.15s/it, train_loss=1.68, valid_loss=2.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "patience_check: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:14<11:14, 26.99s/it, train_loss=1.56, valid_loss=2.46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "patience_check: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:41<10:45, 26.90s/it, train_loss=1.5, valid_loss=2.48] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "patience_check: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [03:08<10:18, 26.88s/it, train_loss=1.47, valid_loss=2.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "patience_check: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [03:35<09:53, 26.97s/it, train_loss=1.44, valid_loss=2.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4598 61359\n",
      "patience_check: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [03:52<10:39, 29.07s/it, train_loss=1.44, valid_loss=2.48]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 30\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m minimum_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(train_user, r_avg)\n",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 30\u001b[0m in \u001b[0;36mDeepCoNN.train\u001b[0;34m(self, train_user, r_avg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(data)\u001b[39m==\u001b[39m\u001b[39m6\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m     fields, target \u001b[39m=\u001b[39m [data[\u001b[39m'\u001b[39m\u001b[39muser_isbn_vector\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), data[\u001b[39m'\u001b[39m\u001b[39muser_summary_merge_vector\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), data[\u001b[39m'\u001b[39m\u001b[39mitem_summary_vector\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)], data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(fields)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(y, target\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 30\u001b[0m in \u001b[0;36m_DeepCoNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m item_text_feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn_i(item_text_vector)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m feature_vector \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m                             user_isbn_feature\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, user_isbn_feature\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m user_isbn_feature\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m                             user_text_feature,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m                             item_text_feature\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m                             ], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfm(feature_vector)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X51sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/nn/modules/module.py:1197\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1196\u001b[0m     _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1197\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _parameters:\n\u001b[1;32m   1198\u001b[0m         \u001b[39mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1199\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_buffers\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "minimum_loss = model.train(train_user, r_avg) # user도 없고, book summary가 none일 때만. 그때 book rating이 있으면 그걸로, 없으면 7.069로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:26<12:42, 26.31s/it, train_loss=13.5, valid_loss=3.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7712 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:53<12:30, 26.79s/it, train_loss=2.42, valid_loss=2.35]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7712 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:19<11:55, 26.49s/it, train_loss=1.9, valid_loss=2.37] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7712 61359\n",
      "patience_check: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:45<11:25, 26.35s/it, train_loss=1.67, valid_loss=2.4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7712 61359\n",
      "patience_check: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:11<10:56, 26.27s/it, train_loss=1.55, valid_loss=2.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7712 61359\n",
      "patience_check: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:20<11:42, 28.09s/it, train_loss=1.55, valid_loss=2.42]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m minimum_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(train_user, r_avg)\n",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 28\u001b[0m in \u001b[0;36mDeepCoNN.train\u001b[0;34m(self, train_user, r_avg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(data)\u001b[39m==\u001b[39m\u001b[39m6\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     fields, target \u001b[39m=\u001b[39m [data[\u001b[39m'\u001b[39m\u001b[39muser_isbn_vector\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), data[\u001b[39m'\u001b[39m\u001b[39muser_summary_merge_vector\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), data[\u001b[39m'\u001b[39m\u001b[39mitem_summary_vector\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)], data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(fields)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(y, target\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 28\u001b[0m in \u001b[0;36m_DeepCoNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m item_text_feature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcnn_i(item_text_vector)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m feature_vector \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m                             user_isbn_feature\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, user_isbn_feature\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m user_isbn_feature\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m                             user_text_feature,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m                             item_text_feature\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m                             ], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfm(feature_vector)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 28\u001b[0m in \u001b[0;36mFactorizationMachine_v.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     linear \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     square_of_sum \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmm(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv) \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     sum_of_square \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmm(x \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     pair_interactions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(square_of_sum \u001b[39m-\u001b[39m sum_of_square, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/_tensor.py:32\u001b[0m, in \u001b[0;36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function(args):\n\u001b[1;32m     31\u001b[0m         \u001b[39mreturn\u001b[39;00m handle_torch_function(wrapped, args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     33\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "minimum_loss = model.train(train_user, r_avg) # 둘 다 cold일 때 7.069 넣어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:26<12:45, 26.41s/it, train_loss=11.7, valid_loss=3.1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:52<12:18, 26.38s/it, train_loss=2.46, valid_loss=2.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "updated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:18<11:44, 26.08s/it, train_loss=1.9, valid_loss=2.34] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [01:44<11:19, 26.13s/it, train_loss=1.67, valid_loss=2.38]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:10<10:52, 26.10s/it, train_loss=1.56, valid_loss=2.42]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [02:36<10:25, 26.05s/it, train_loss=1.51, valid_loss=2.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [03:01<09:57, 25.99s/it, train_loss=1.47, valid_loss=2.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 61359\n",
      "patience_check: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [03:16<10:45, 28.06s/it, train_loss=1.47, valid_loss=2.43]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 28\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m minimum_loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(train_user, r_avg)\n",
      "\u001b[1;32m/opt/ml/input/good_ipynb.ipynb 셀 28\u001b[0m in \u001b[0;36mDeepCoNN.train\u001b[0;34m(self, train_user, r_avg)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39muser_isbn_vector\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baistage/opt/ml/input/good_ipynb.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m/opt/conda/envs/dongyoung/lib/python3.10/site-packages/torch/optim/adam.py:259\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    256\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39;49madd(param, alpha\u001b[39m=\u001b[39;49mweight_decay)\n\u001b[1;32m    261\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    262\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "minimum_loss = model.train(train_user, r_avg) # 아무것도 안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 76699\n"
     ]
    }
   ],
   "source": [
    "predicts  = model.predict(data['test_dataloader'], train_user, r_avg) # data에 test loader가 있으므로 받지 않아도 무방할듯..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(args.DATA_PATH + 'sample_submission.csv')\n",
    "submission['rating'] = predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(DATA_PATH='/opt/ml/input/code/data/', MODEL='MIX', DATA_SHUFFLE=True, TEST_SIZE=0.2, SEED=42, BATCH_SIZE=32, EPOCHS=30, LR=0.001, WEIGHT_DECAY=0.0004, DEVICE='cuda', FM_EMBED_DIM=16, FFM_EMBED_DIM=16, NCF_EMBED_DIM=16, NCF_MLP_DIMS=(16, 16), NCF_DROPOUT=0.2, WDN_EMBED_DIM=16, WDN_MLP_DIMS=(16, 16), WDN_DROPOUT=0.2, DCN_EMBED_DIM=16, DCN_MLP_DIMS=(16, 16), DCN_DROPOUT=0.2, DCN_NUM_LAYERS=3, CNN_FM_EMBED_DIM=128, CNN_FM_LATENT_DIM=8, DEEPCONN_VECTOR_CREATE=False, DEEPCONN_EMBED_DIM=8, DEEPCONN_LATENT_DIM=8, DEEPCONN_CONV_1D_OUT_DIM=16, DEEPCONN_KERNEL_SIZE=3, DEEPCONN_WORD_DIM=768, DEEPCONN_OUT_DIM=16, model=<class '__main__.MIX'>)\n"
     ]
    }
   ],
   "source": [
    "args.MODEL = 'MIX'\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_loss = 2.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('/opt/ml/input/code/submit/{}_{}.csv'.format('_'.join([f'{args.MODEL}', f'{args.BATCH_SIZE}', f'{args.LR}', f'{args.WEIGHT_DECAY}']), minimum_loss), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dongyoung')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f003a16f78e953d6c3fb089b790d98e4533a1b4d943e2d8ed222c282940149d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
